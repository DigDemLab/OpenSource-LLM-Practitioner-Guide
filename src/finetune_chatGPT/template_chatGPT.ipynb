{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template for Fine-Tuning Classification with OpenAI Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pprint\n",
    "import time\n",
    "import argparse\n",
    "from ast import literal_eval\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "from utils import (\n",
    "    dataset_has_format_errors,\n",
    "    write_jsonl,\n",
    ")\n",
    "from utils_src import task_num_to_task_name, dataset_num_to_dataset_name, plot_count_and_normalized_confusion_matrix, \\\n",
    "    task_to_display_labels, load_dataset_task_prompt_mappings\n",
    "\n",
    "module_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# read API key\n",
    "with open('src/OpenAI_key.txt') as f:\n",
    "    openai.api_key = f.readlines()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Arguments and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block, you are required to set up several key parameters that will define the behavior and environment of your fine-tuning process:\n",
    "\n",
    "1. **WandB Project Name (`WANDB_PROJECT_NAME`)**: This is the name of the project in Weights & Biases (WandB) where your training run will be logged. WandB is a tool that helps track experiments, visualize data, and share insights. By setting the project name here, you ensure that all the metrics, outputs, and logs from your training process are organized under a single project for easy access and comparison. Specify a meaningful name that reflects the nature of your training session or experiment.\n",
    "\n",
    "2. **Model Name (`MODEL_NAME`)**: Here, you select the specific model from OpenAI's suite that you wish to fine-tune. The model name, such as `'gpt-3.5-turbo-0613'`, refers to a particular configuration and version of the model. This selection dictates the starting point of your fine-tuning process, leveraging the pre-trained weights and architecture of the specified model. Ensure that the model name corresponds to an existing and available model in OpenAI's library. You can find more models here:  https://platform.openai.com/docs/models \n",
    "\n",
    "3. **Completion Retries (`COMPLETION_RETRIES`)**: This parameter defines the number of retry attempts for generating a completion in case the initial attempts fail. When interacting with the model, especially in a fine-tuning context, certain queries may not succeed on the first try due to various reasons (e.g., network issues, API errors). This setting provides resilience, allowing the process to attempt the generation multiple times before considering it a failure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specs WandB and Which Model you want to fine-tune\n",
    "WANDB_PROJECT_NAME = \"chatGPT_template_1\"\n",
    "MODEL_NAME = 'gpt-3.5-turbo-0613'\n",
    "COMPLETION_RETRIES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code block, you are required to set up various configuration variables that will dictate how the inference processes are executed. These variables are crucial as they define the nature of the task, the data, and the specific behaviors during the model's training and evaluation.\n",
    "\n",
    "1. **Task (`task`)**: Specify the type of task you want to run inference on. The task is represented by an integer, with each number corresponding to a different type of task (e.g., 1, 2, 3, etc.). You must select from the predefined choices, which are typically mapped to specific NLP tasks or scenarios.\n",
    "\n",
    "2. **Dataset (`dataset`)**: Choose the dataset on which you want to run inference. Like tasks, datasets are identified by integers, and each number corresponds to a different dataset. Ensure that the dataset selected is relevant to the task at hand.\n",
    "\n",
    "3. **Output Directory (`output_dir`)**: Define the path to the directory where you want to store the generated samples. This is where the output of your training and inference processes will be saved.\n",
    "\n",
    "4. **Random Seed (`seed`)**: Setting a random seed ensures that the results are reproducible. By using the same seed, you can achieve the same outcomes on repeated runs under identical conditions.\n",
    "\n",
    "5. **Data Directory (`data_dir`)**: Specify the path to the directory containing the datasets you plan to use for training and evaluation.\n",
    "\n",
    "6. **Label Usage (`not_use_full_labels`)**: This boolean variable determines whether to use the full label descriptions or abbreviated labels during training and inference. Setting it to `False` means full labels will be used.\n",
    "\n",
    "7. **Dataset-Task Mappings File Path (`dataset_task_mappings_fp`)**: Define the path to the file containing mappings between datasets and tasks. This file is crucial for ensuring the correct dataset is used for the specified task.\n",
    "\n",
    "8. **Rewrite DataFrame (`rewrite_df_in_openai`)**: A boolean that dictates whether to rewrite the dataframe in OpenAI format. If set to `True`, the data will be reformatted according to OpenAI's expected input structure during the execution.\n",
    "\n",
    "9. **Number of Epochs (`n_epochs`)**: Specify the number of epochs for training the model. An epoch refers to one complete pass through the entire training dataset.\n",
    "\n",
    "10. **Run Name (`run_name`)**: Give a unique name to your run, which will help you identify it later, especially when tracking multiple experiments or runs.\n",
    "\n",
    "11. **Temperature (`temp`)**: Set the temperature for text generation. Temperature controls the randomness of the output; a lower temperature results in less random completions.\n",
    "\n",
    "12. **Few-shot (`few_shot`)**: A boolean indicating whether to use a few-shot learning approach. When set to `True`, the model will be fine-tuned with only a few examples.\n",
    "\n",
    "13. **System-User Prompt Division (`system_user_division`)**: This integer defines the separation between system and user prompts, which is critical for structuring the input data correctly for the model.\n",
    "\n",
    "**Customizing for Your Own Tasks:**\n",
    "If you plan to run a custom task or use a dataset that is not predefined, you will need to make modifications to the `utils_src` file. This file contains all mappings for different datasets and tasks. Adding your custom task or dataset involves defining the new task or dataset number and specifying its characteristics and mappings in the `utils_src` file. This ensures that your custom task or dataset integrates seamlessly with the existing framework for training and inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Variables\n",
    "\n",
    "# Type of task to run inference on\n",
    "task = 1  # Choices: [1,2,3,4,5,6]\n",
    "\n",
    "# Dataset to run inference on\n",
    "dataset = 1  # Choices: [1, 2, 3, 4]\n",
    "\n",
    "# Size of the sample to generate\n",
    "sample_size = '250'  # Choices: ['50','100','250','500','1000','1500']\n",
    "\n",
    "# Path to the directory to store the generated samples\n",
    "output_dir = '../../data'\n",
    "\n",
    "# Random seed to use\n",
    "seed = 2019\n",
    "\n",
    "# Path to the directory containing the datasets\n",
    "data_dir = '../../data'\n",
    "\n",
    "# Whether to use the full label\n",
    "not_use_full_labels = False\n",
    "\n",
    "# Path to the dataset-task mappings file\n",
    "dataset_task_mappings_fp = os.path.normpath(os.path.join(module_dir, '..', 'dataset_task_mappings.csv'))\n",
    "\n",
    "# Whether to rewrite the dataframe in OpenAI format\n",
    "rewrite_df_in_openai = True\n",
    "\n",
    "# Number of epochs to train the model\n",
    "n_epochs = 3\n",
    "\n",
    "# Name of the run\n",
    "run_name = 'finetune_chatGPT_3.5_template'\n",
    "\n",
    "# Temperature to use when generating text\n",
    "temp = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_and_eval_sets(data_dir: str, dataset_num: int, task_num: int, sample_size: int, dataset_eval:str) \\\n",
    "        -> dict[str, pd.DataFrame]:\n",
    "    datasets = dict()\n",
    "\n",
    "    train_dataset_task_files = glob.glob(os.path.join(data_dir, f'ds_{dataset_num}__task_{task_num}_train_set*.csv'))\n",
    "    eval_set_name = f'ds_{dataset_num}__task_{task_num}_eval_set'\n",
    "    datasets[eval_set_name] = pd.read_csv(os.path.join(data_dir, eval_set_name + '.csv'))\n",
    "\n",
    "    # Load the additional evaluation dataset specified by dataset_eval\n",
    "    second_eval_set_name = f'ds_{dataset_eval}__task_{task_num}_full_eval'\n",
    "    datasets[second_eval_set_name] = pd.read_csv(os.path.join(data_dir, second_eval_set_name + '.csv'))\n",
    "\n",
    "    if sample_size == 'all':\n",
    "        train_dfs_ = {fn.strip('.csv'): pd.read_csv(fn) for fn in train_dataset_task_files}\n",
    "        datasets.update(train_dfs_)\n",
    "    else:\n",
    "        train_df_fn = f'ds_{dataset_num}__task_{task_num}_train_set_{sample_size}'\n",
    "        datasets[train_df_fn] = pd.read_csv(os.path.join(data_dir, train_df_fn + '.csv'))\n",
    "\n",
    "        if train_df_fn not in [os.path.basename(fn).strip('.csv') for fn in train_dataset_task_files]:\n",
    "            raise ValueError(f\"Sample size {sample_size} not found for\"\n",
    "                             f\" dataset {dataset_num} and task {task_num}\")\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_example(system_prompt, user_prompt_format, user_prompt_text, completion):\n",
    "    return {'messages': [\n",
    "        {'role': 'system',\n",
    "         'content': system_prompt},\n",
    "\n",
    "        {'role': 'user',\n",
    "         'content': user_prompt_format.format(text=user_prompt_text)},\n",
    "\n",
    "        {'role': 'assistant',\n",
    "         'content': completion}\n",
    "    ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_datasets_to_openai(output_dir, not_use_full_labels, rewrite_df_in_openai, datasets):\n",
    "    hatespeech_open_ai_metadata = list()\n",
    "\n",
    "    df_id_metadata = pd.DataFrame() if not os.path.exists('hatespeech_open_ai_metadata.csv') \\\n",
    "        else pd.read_csv('hatespeech_open_ai_metadata.csv')\n",
    "\n",
    "    for df_name, df in datasets.items():\n",
    "        df_jsonl_filename = os.path.join(output_dir, 'temp', df_name + '.jsonl')\n",
    "        write_jsonl(data_list=df['openai_instance_format'].tolist(), filename=df_jsonl_filename)\n",
    "\n",
    "        if not_use_full_labels:\n",
    "            df_name += '_single_letter_labels'\n",
    "\n",
    "        if (not rewrite_df_in_openai and\n",
    "                (len(df_id_metadata) > 0 and df_name in df_id_metadata['df_name'].tolist())):\n",
    "            print(f\"Dataset {df_name} already uploaded to OpenAI\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Uploading {df_name} to OpenAI\")\n",
    "        df_response = openai.File.create(\n",
    "            file=open(df_jsonl_filename, \"rb\"), purpose=\"fine-tune\"\n",
    "        )\n",
    "        df_file_id = df_response[\"id\"]\n",
    "\n",
    "        # Wait until the file is processed\n",
    "        while True:\n",
    "            file = openai.File.retrieve(df_file_id)\n",
    "            if file[\"status\"] == \"processed\":\n",
    "                break\n",
    "            time.sleep(15)\n",
    "        hatespeech_open_ai_metadata.append({'df_name': df_name, 'file_id': df_file_id})\n",
    "\n",
    "    df_id_metadata = pd.concat([df_id_metadata, pd.DataFrame(hatespeech_open_ai_metadata)])\n",
    "    df_id_metadata.to_csv('hatespeech_open_ai_metadata.csv', index=False)\n",
    "\n",
    "    return df_id_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_chat_gpt(evaluation_file_id, training_file_id, model_name, n_epochs):\n",
    "    response = openai.FineTuningJob.create(\n",
    "        training_file=training_file_id,\n",
    "        validation_file=evaluation_file_id,\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        suffix=model_name,\n",
    "        hyperparameters={\"n_epochs\": n_epochs}\n",
    "    )\n",
    "\n",
    "    job_id = response[\"id\"]\n",
    "    print(\"Job ID:\", response[\"id\"])\n",
    "    print(\"Status:\", response[\"status\"])\n",
    "\n",
    "    # Wait until the job is done\n",
    "    while True:\n",
    "        job = openai.FineTuningJob.retrieve(job_id)\n",
    "        if job[\"status\"] == \"succeeded\":\n",
    "            break\n",
    "        elif job[\"status\"] == \"failed\":\n",
    "            raise Exception(\"Training failed: %s\" % job[\"error\"])\n",
    "        time.sleep(30)\n",
    "\n",
    "    return job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_and_log_finetuning_event_history(job_id):\n",
    "    response = openai.FineTuningJob.list_events(id=job_id)\n",
    "    events = response[\"data\"]\n",
    "    events.reverse()\n",
    "    for event in events:\n",
    "        print(event[\"message\"])\n",
    "\n",
    "    # Log events\n",
    "    for event in events:\n",
    "        if event['type'] != 'metrics':\n",
    "            continue\n",
    "        data = event['data']\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train_loss\": data[\"train_loss\"],\n",
    "                \"valid_loss\": data[\"valid_loss\"],\n",
    "                \"train_mean_token_accuracy\": data[\"train_mean_token_accuracy\"],\n",
    "                \"valid_mean_token_accuracy\": data[\"valid_mean_token_accuracy\"]\n",
    "            },\n",
    "            step=data['step']\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Weights and Biases run\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=WANDB_PROJECT_NAME,\n",
    "    name=run_name if run_name != '' else f'{MODEL_NAME}_ds_{dataset}_task_{int(task)}'\n",
    "                                                    f'_sample_{sample_size}_epochs_{n_epochs}'\n",
    "                                                    f'_full_label_names_{str(not not_use_full_labels)}'\n",
    "                                                    f'_temp_{temp}',\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"dataset\": dataset_num_to_dataset_name[int(dataset)],\n",
    "        \"task\": task_num_to_task_name[int(task)],\n",
    "        \"epochs\": n_epochs,\n",
    "        \"temp\": temp\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and filename\n",
    "dataset_idx, dataset_task_mappings = load_dataset_task_prompt_mappings(\n",
    "    dataset_num=dataset, task_num=task, dataset_task_mappings_fp=dataset_task_mappings_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and eval datasets\n",
    "datasets = load_train_and_eval_sets(\n",
    "    data_dir=data_dir, dataset_num=dataset, task_num=task, sample_size=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information specific to the dataset\n",
    "label_column = dataset_task_mappings.loc[dataset_idx, \"label_column\"]\n",
    "system_prompt = dataset_task_mappings.loc[dataset_idx, 'zero_shot_prompt']\n",
    "user_prompt_format = dataset_task_mappings.loc[dataset_idx, 'user_prompt']\n",
    "\n",
    "#system_user_prompt_division_line = 3 if args.task != 3 else 15\n",
    "system_user_prompt_division_line = args.system_user_division\n",
    "system_prompt = ('\\n'.join(prompt.split('\\n')[:-system_user_prompt_division_line])).strip()\n",
    "user_prompt_format = ('\\n'.join(prompt.split('\\n')[-system_user_prompt_division_line:])).strip()\n",
    "print(user_prompt_format)\n",
    "\n",
    "\n",
    "# Log the system prompt and user_prompt_format as files in wandb\n",
    "prompts_artifact = wandb.Artifact('prompts', type='prompts')\n",
    "with prompts_artifact.new_file('system_prompt.txt', mode='w', encoding='utf-8') as f:\n",
    "    f.write(system_prompt)\n",
    "with prompts_artifact.new_file('user_prompt_format.txt', mode='w', encoding='utf-8') as f:\n",
    "    f.write(user_prompt_format)\n",
    "wandb.run.log_artifact(prompts_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training and evaluation examples in the way expected by the Open AI API to finetune chatgpt3.5\n",
    "preprocessed_output_dir = os.path.join(\n",
    "    output_dir, 'preprocessed', 'full_name_labels' if not not_use_full_labels else 'single_letter_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in datasets.items():\n",
    "    df['completion_label'] = df[label_column].map(\n",
    "        lambda label: map_label_to_completion(label=label, task_num = task,\n",
    "                                              full_label=not not_use_full_labels)\n",
    "        )\n",
    "    df['openai_instance_format'] = df.apply(\n",
    "        lambda row: create_training_example(\n",
    "            system_prompt=system_prompt, user_prompt_format=user_prompt_format,\n",
    "            user_prompt_text=row['text'],\n",
    "            completion=row['completion_label']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    df['openai_instance_without_completion'] = df['openai_instance_format'].map(lambda x: x['messages'][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Check for errors {df_name} set: ')\n",
    "assert not dataset_has_format_errors(df['openai_instance_format'].tolist()), f\"Errors found in {df_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(preprocessed_output_dir, df_name + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Training Dataset to Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create jsonl file and upload to OpenAI\n",
    "df_id_metadata =upload_datasets_to_openai(output_dir, not_use_full_labels, rewrite_df_in_openai, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all files in the temp folder\n",
    "os.system(f\"rm -rf {os.path.join(output_dir, 'temp')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune chatGPT Model with Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training on the train_df samples selected\n",
    "eval_set_name = f'ds_{dataset}__task_{task}_eval_set'\n",
    "eval_df = datasets[eval_set_name]\n",
    "for df_name, df in datasets.items():\n",
    "    if df_name == eval_set_name:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Finetuning {df_name}\")\n",
    "print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in wandb.config the dataset sample size used\n",
    "sample_size = df_name.split('_')[-1]\n",
    "wandb.config['trainset_size'] = sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the fine-tuning job\n",
    "training_file_id = df_id_metadata.loc[df_id_metadata['df_name'] == df_name, 'file_id'].values[0]\n",
    "evaluation_file_id = df_id_metadata.loc[df_id_metadata['df_name'] == eval_set_name, 'file_id'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = (df_name.replace('__', '_')\n",
    "              .replace('train_set', 'trn')\n",
    "              .replace('task', 't')\n",
    "              .replace('_single_letter_labels', '_sl'))\n",
    "job_id = fine_tune_chat_gpt(evaluation_file_id, training_file_id, model_name=model_name, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model name\n",
    "response = openai.FineTuningJob.retrieve(job_id)\n",
    "full_model_name = response[\"fine_tuned_model\"]\n",
    "print(f'The model {full_model_name} has been successfully fine-tuned')\n",
    "wandb.config['model_name_openai'] = full_model_name\n",
    "wandb.config['finetuning_jobid'] = job_id\n",
    "wandb.config['training_file_openai_id'] = response['training_file']\n",
    "wandb.config['validation_file_openai_id'] = response['validation_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the events (training history of the model)\n",
    "print_and_log_finetuning_event_history(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Evaluate the model on the evaluation set and store the predictions\n",
    "print(\"\\n\" + \"#\" * 50)\n",
    "print(\"Getting predictions on the evaluation set\")\n",
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for messages in tqdm(eval_df['openai_instance_without_completion'].tolist()):\n",
    "    # Retry the completion at least COMPLETION_RETRIES times\n",
    "    num_retries = 2\n",
    "    response = None\n",
    "    while num_retries < COMPLETION_RETRIES and response is None:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=full_model_name,\n",
    "                messages=messages,\n",
    "                temperature=temp,\n",
    "                n=1\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print('Error getting predictions. Retrying...')\n",
    "            time.sleep(5)\n",
    "            num_retries += 1\n",
    "            if num_retries >= COMPLETION_RETRIES:\n",
    "                print('Maximum amount of retires reached')\n",
    "                raise e\n",
    "    predictions.append(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to df\n",
    "eval_df['prediction'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store output\n",
    "predictions_output_dir = os.path.join(output_dir, 'predictions',\n",
    "                                      f'dataset_{dataset}_task_{task}')\n",
    "os.makedirs(predictions_output_dir, exist_ok=True)\n",
    "datasets[eval_set_name].to_csv(\n",
    "    os.path.join(predictions_output_dir, f\"{model_name}-{run_name}.csv\"),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get performance metrics\n",
    "y_true = eval_df['completion_label']\n",
    "y_pred = eval_df['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_type = 'full_name' if not not_use_full_labels else 'short_name'\n",
    "display_labels = task_to_display_labels[task][label_type]\n",
    "labels = display_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_plot, classification_report, metrics = plot_count_and_normalized_confusion_matrix(\n",
    "    y_true, y_pred, display_labels, labels, xticks_rotation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log metrics\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    wandb.log({metric_name: metric_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the confusion matrix matplotlib figure\n",
    "wandb.log({'confusion_matrix': wandb.Image(cm_plot)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the classification report as an artifact\n",
    "classification_report = (pd.DataFrame({k: v for k, v in classification_report.items() if k != 'accuracy'})\n",
    "                         .transpose().reset_index())\n",
    "\n",
    "wandb.log({'classification_report': wandb.Table(\n",
    "    dataframe=classification_report)})\n",
    "\n",
    "classification_report_artifact = wandb.Artifact(\n",
    "      f'classification_report_{model_name}', type='classification_report')\n",
    "\n",
    "with classification_report_artifact.new_file('classification_report.txt', mode='w') as f:\n",
    "    f.write(pprint.pformat(classification_report))\n",
    "\n",
    "wandb.run.log_artifact(classification_report_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminate WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark the end of the run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
