{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template for Fine-Tuning Classification with OpenAI Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pprint\n",
    "import time\n",
    "import argparse\n",
    "from ast import literal_eval\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "from utils import (\n",
    "    dataset_has_format_errors,\n",
    "    write_jsonl,\n",
    ")\n",
    "from utils_src import task_num_to_task_name, dataset_num_to_dataset_name, plot_count_and_normalized_confusion_matrix, \\\n",
    "    task_to_display_labels, load_dataset_task_prompt_mappings\n",
    "\n",
    "module_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# read API key\n",
    "with open('src/OpenAI_key.txt') as f:\n",
    "    openai.api_key = f.readlines()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Arguments and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specs WandB and Which Model you want to fine-tune\n",
    "WANDB_PROJECT_NAME = \"chatGPT_template_1\"\n",
    "MODEL_NAME = 'gpt-3.5-turbo-0613'\n",
    "COMPLETION_RETRIES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Variables\n",
    "\n",
    "# Type of task to run inference on\n",
    "task = 1  # Choices: [1,2,3,4,5,6]\n",
    "\n",
    "# Dataset to run inference on\n",
    "dataset = 1  # Choices: [1, 2, 3, 4]\n",
    "\n",
    "# Size of the sample to generate\n",
    "sample_size = '250'  # Choices: ['50','100','250','500','1000','1500']\n",
    "\n",
    "# Path to the directory to store the generated samples\n",
    "output_dir = '../../data'\n",
    "\n",
    "# Random seed to use\n",
    "seed = 2019\n",
    "\n",
    "# Path to the directory containing the datasets\n",
    "data_dir = '../../data'\n",
    "\n",
    "# Whether to use the full label\n",
    "not_use_full_labels = False\n",
    "\n",
    "# Path to the dataset-task mappings file\n",
    "dataset_task_mappings_fp = os.path.normpath(os.path.join(module_dir, '..', 'dataset_task_mappings.csv'))\n",
    "\n",
    "# Whether to rewrite the dataframe in OpenAI format\n",
    "rewrite_df_in_openai = True\n",
    "\n",
    "# Number of epochs to train the model\n",
    "n_epochs = 3\n",
    "\n",
    "# Name of the run\n",
    "run_name = 'finetune_chatGPT_3.5_template'\n",
    "\n",
    "# Temperature to use when generating text\n",
    "temp = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Unitily Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_and_eval_sets(data_dir: str, dataset_num: int, task_num: int, sample_size: int, dataset_eval:str) \\\n",
    "        -> dict[str, pd.DataFrame]:\n",
    "    datasets = dict()\n",
    "\n",
    "    train_dataset_task_files = glob.glob(os.path.join(data_dir, f'ds_{dataset_num}__task_{task_num}_train_set*.csv'))\n",
    "    eval_set_name = f'ds_{dataset_num}__task_{task_num}_eval_set'\n",
    "    datasets[eval_set_name] = pd.read_csv(os.path.join(data_dir, eval_set_name + '.csv'))\n",
    "\n",
    "    # Load the additional evaluation dataset specified by dataset_eval\n",
    "    second_eval_set_name = f'ds_{dataset_eval}__task_{task_num}_full_eval'\n",
    "    datasets[second_eval_set_name] = pd.read_csv(os.path.join(data_dir, second_eval_set_name + '.csv'))\n",
    "\n",
    "    if sample_size == 'all':\n",
    "        train_dfs_ = {fn.strip('.csv'): pd.read_csv(fn) for fn in train_dataset_task_files}\n",
    "        datasets.update(train_dfs_)\n",
    "    else:\n",
    "        train_df_fn = f'ds_{dataset_num}__task_{task_num}_train_set_{sample_size}'\n",
    "        datasets[train_df_fn] = pd.read_csv(os.path.join(data_dir, train_df_fn + '.csv'))\n",
    "\n",
    "        if train_df_fn not in [os.path.basename(fn).strip('.csv') for fn in train_dataset_task_files]:\n",
    "            raise ValueError(f\"Sample size {sample_size} not found for\"\n",
    "                             f\" dataset {dataset_num} and task {task_num}\")\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_example(system_prompt, user_prompt_format, user_prompt_text, completion):\n",
    "    return {'messages': [\n",
    "        {'role': 'system',\n",
    "         'content': system_prompt},\n",
    "\n",
    "        {'role': 'user',\n",
    "         'content': user_prompt_format.format(text=user_prompt_text)},\n",
    "\n",
    "        {'role': 'assistant',\n",
    "         'content': completion}\n",
    "    ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_datasets_to_openai(output_dir, not_use_full_labels, rewrite_df_in_openai, datasets):\n",
    "    hatespeech_open_ai_metadata = list()\n",
    "\n",
    "    df_id_metadata = pd.DataFrame() if not os.path.exists('hatespeech_open_ai_metadata.csv') \\\n",
    "        else pd.read_csv('hatespeech_open_ai_metadata.csv')\n",
    "\n",
    "    for df_name, df in datasets.items():\n",
    "        df_jsonl_filename = os.path.join(output_dir, 'temp', df_name + '.jsonl')\n",
    "        write_jsonl(data_list=df['openai_instance_format'].tolist(), filename=df_jsonl_filename)\n",
    "\n",
    "        if not_use_full_labels:\n",
    "            df_name += '_single_letter_labels'\n",
    "\n",
    "        if (not rewrite_df_in_openai and\n",
    "                (len(df_id_metadata) > 0 and df_name in df_id_metadata['df_name'].tolist())):\n",
    "            print(f\"Dataset {df_name} already uploaded to OpenAI\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Uploading {df_name} to OpenAI\")\n",
    "        df_response = openai.File.create(\n",
    "            file=open(df_jsonl_filename, \"rb\"), purpose=\"fine-tune\"\n",
    "        )\n",
    "        df_file_id = df_response[\"id\"]\n",
    "\n",
    "        # Wait until the file is processed\n",
    "        while True:\n",
    "            file = openai.File.retrieve(df_file_id)\n",
    "            if file[\"status\"] == \"processed\":\n",
    "                break\n",
    "            time.sleep(15)\n",
    "        hatespeech_open_ai_metadata.append({'df_name': df_name, 'file_id': df_file_id})\n",
    "\n",
    "    df_id_metadata = pd.concat([df_id_metadata, pd.DataFrame(hatespeech_open_ai_metadata)])\n",
    "    df_id_metadata.to_csv('hatespeech_open_ai_metadata.csv', index=False)\n",
    "\n",
    "    return df_id_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_chat_gpt(evaluation_file_id, training_file_id, model_name, n_epochs):\n",
    "    response = openai.FineTuningJob.create(\n",
    "        training_file=training_file_id,\n",
    "        validation_file=evaluation_file_id,\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        suffix=model_name,\n",
    "        hyperparameters={\"n_epochs\": n_epochs}\n",
    "    )\n",
    "\n",
    "    job_id = response[\"id\"]\n",
    "    print(\"Job ID:\", response[\"id\"])\n",
    "    print(\"Status:\", response[\"status\"])\n",
    "\n",
    "    # Wait until the job is done\n",
    "    while True:\n",
    "        job = openai.FineTuningJob.retrieve(job_id)\n",
    "        if job[\"status\"] == \"succeeded\":\n",
    "            break\n",
    "        elif job[\"status\"] == \"failed\":\n",
    "            raise Exception(\"Training failed: %s\" % job[\"error\"])\n",
    "        time.sleep(30)\n",
    "\n",
    "    return job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_and_log_finetuning_event_history(job_id):\n",
    "    response = openai.FineTuningJob.list_events(id=job_id)\n",
    "    events = response[\"data\"]\n",
    "    events.reverse()\n",
    "    for event in events:\n",
    "        print(event[\"message\"])\n",
    "\n",
    "    # Log events\n",
    "    for event in events:\n",
    "        if event['type'] != 'metrics':\n",
    "            continue\n",
    "        data = event['data']\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train_loss\": data[\"train_loss\"],\n",
    "                \"valid_loss\": data[\"valid_loss\"],\n",
    "                \"train_mean_token_accuracy\": data[\"train_mean_token_accuracy\"],\n",
    "                \"valid_mean_token_accuracy\": data[\"valid_mean_token_accuracy\"]\n",
    "            },\n",
    "            step=data['step']\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Weights and Biases run\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=WANDB_PROJECT_NAME,\n",
    "    name=run_name if run_name != '' else f'{MODEL_NAME}_ds_{dataset}_task_{int(task)}'\n",
    "                                                    f'_sample_{sample_size}_epochs_{n_epochs}'\n",
    "                                                    f'_full_label_names_{str(not not_use_full_labels)}'\n",
    "                                                    f'_temp_{temp}',\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"dataset\": dataset_num_to_dataset_name[int(dataset)],\n",
    "        \"task\": task_num_to_task_name[int(task)],\n",
    "        \"epochs\": n_epochs,\n",
    "        \"temp\": temp\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and filename\n",
    "dataset_idx, dataset_task_mappings = load_dataset_task_prompt_mappings(\n",
    "    dataset_num=dataset, task_num=task, dataset_task_mappings_fp=dataset_task_mappings_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and eval datasets\n",
    "datasets = load_train_and_eval_sets(\n",
    "    data_dir=data_dir, dataset_num=dataset, task_num=task, sample_size=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information specific to the dataset\n",
    "label_column = dataset_task_mappings.loc[dataset_idx, \"label_column\"]\n",
    "system_prompt = dataset_task_mappings.loc[dataset_idx, 'zero_shot_prompt']\n",
    "user_prompt_format = dataset_task_mappings.loc[dataset_idx, 'user_prompt']\n",
    "\n",
    "#system_user_prompt_division_line = 3 if args.task != 3 else 15\n",
    "system_user_prompt_division_line = args.system_user_division\n",
    "system_prompt = ('\\n'.join(prompt.split('\\n')[:-system_user_prompt_division_line])).strip()\n",
    "user_prompt_format = ('\\n'.join(prompt.split('\\n')[-system_user_prompt_division_line:])).strip()\n",
    "print(user_prompt_format)\n",
    "\n",
    "\n",
    "# Log the system prompt and user_prompt_format as files in wandb\n",
    "prompts_artifact = wandb.Artifact('prompts', type='prompts')\n",
    "with prompts_artifact.new_file('system_prompt.txt', mode='w', encoding='utf-8') as f:\n",
    "    f.write(system_prompt)\n",
    "with prompts_artifact.new_file('user_prompt_format.txt', mode='w', encoding='utf-8') as f:\n",
    "    f.write(user_prompt_format)\n",
    "wandb.run.log_artifact(prompts_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training and evaluation examples in the way expected by the Open AI API to finetune chatgpt3.5\n",
    "preprocessed_output_dir = os.path.join(\n",
    "    output_dir, 'preprocessed', 'full_name_labels' if not not_use_full_labels else 'single_letter_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in datasets.items():\n",
    "    df['completion_label'] = df[label_column].map(\n",
    "        lambda label: map_label_to_completion(label=label, task_num = task,\n",
    "                                              full_label=not not_use_full_labels)\n",
    "        )\n",
    "    df['openai_instance_format'] = df.apply(\n",
    "        lambda row: create_training_example(\n",
    "            system_prompt=system_prompt, user_prompt_format=user_prompt_format,\n",
    "            user_prompt_text=row['text'],\n",
    "            completion=row['completion_label']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    df['openai_instance_without_completion'] = df['openai_instance_format'].map(lambda x: x['messages'][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Check for errors {df_name} set: ')\n",
    "assert not dataset_has_format_errors(df['openai_instance_format'].tolist()), f\"Errors found in {df_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(preprocessed_output_dir, df_name + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Training Dataset to Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create jsonl file and upload to OpenAI\n",
    "df_id_metadata =upload_datasets_to_openai(output_dir, not_use_full_labels, rewrite_df_in_openai, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all files in the temp folder\n",
    "os.system(f\"rm -rf {os.path.join(output_dir, 'temp')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune chatGPT Model with Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training on the train_df samples selected\n",
    "eval_set_name = f'ds_{dataset}__task_{task}_eval_set'\n",
    "eval_df = datasets[eval_set_name]\n",
    "for df_name, df in datasets.items():\n",
    "    if df_name == eval_set_name:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Finetuning {df_name}\")\n",
    "print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in wandb.config the dataset sample size used\n",
    "sample_size = df_name.split('_')[-1]\n",
    "wandb.config['trainset_size'] = sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the fine-tuning job\n",
    "training_file_id = df_id_metadata.loc[df_id_metadata['df_name'] == df_name, 'file_id'].values[0]\n",
    "evaluation_file_id = df_id_metadata.loc[df_id_metadata['df_name'] == eval_set_name, 'file_id'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = (df_name.replace('__', '_')\n",
    "              .replace('train_set', 'trn')\n",
    "              .replace('task', 't')\n",
    "              .replace('_single_letter_labels', '_sl'))\n",
    "job_id = fine_tune_chat_gpt(evaluation_file_id, training_file_id, model_name=model_name, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model name\n",
    "response = openai.FineTuningJob.retrieve(job_id)\n",
    "full_model_name = response[\"fine_tuned_model\"]\n",
    "print(f'The model {full_model_name} has been successfully fine-tuned')\n",
    "wandb.config['model_name_openai'] = full_model_name\n",
    "wandb.config['finetuning_jobid'] = job_id\n",
    "wandb.config['training_file_openai_id'] = response['training_file']\n",
    "wandb.config['validation_file_openai_id'] = response['validation_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the events (training history of the model)\n",
    "print_and_log_finetuning_event_history(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Evaluate the model on the evaluation set and store the predictions\n",
    "print(\"\\n\" + \"#\" * 50)\n",
    "print(\"Getting predictions on the evaluation set\")\n",
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for messages in tqdm(eval_df['openai_instance_without_completion'].tolist()):\n",
    "    # Retry the completion at least COMPLETION_RETRIES times\n",
    "    num_retries = 2\n",
    "    response = None\n",
    "    while num_retries < COMPLETION_RETRIES and response is None:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=full_model_name,\n",
    "                messages=messages,\n",
    "                temperature=temp,\n",
    "                n=1\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print('Error getting predictions. Retrying...')\n",
    "            time.sleep(5)\n",
    "            num_retries += 1\n",
    "            if num_retries >= COMPLETION_RETRIES:\n",
    "                print('Maximum amount of retires reached')\n",
    "                raise e\n",
    "    predictions.append(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to df\n",
    "eval_df['prediction'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store output\n",
    "predictions_output_dir = os.path.join(output_dir, 'predictions',\n",
    "                                      f'dataset_{dataset}_task_{task}')\n",
    "os.makedirs(predictions_output_dir, exist_ok=True)\n",
    "datasets[eval_set_name].to_csv(\n",
    "    os.path.join(predictions_output_dir, f\"{model_name}-{run_name}.csv\"),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get performance metrics\n",
    "y_true = eval_df['completion_label']\n",
    "y_pred = eval_df['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_type = 'full_name' if not not_use_full_labels else 'short_name'\n",
    "display_labels = task_to_display_labels[task][label_type]\n",
    "labels = display_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_plot, classification_report, metrics = plot_count_and_normalized_confusion_matrix(\n",
    "    y_true, y_pred, display_labels, labels, xticks_rotation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log metrics\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    wandb.log({metric_name: metric_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the confusion matrix matplotlib figure\n",
    "wandb.log({'confusion_matrix': wandb.Image(cm_plot)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the classification report as an artifact\n",
    "classification_report = (pd.DataFrame({k: v for k, v in classification_report.items() if k != 'accuracy'})\n",
    "                         .transpose().reset_index())\n",
    "\n",
    "wandb.log({'classification_report': wandb.Table(\n",
    "    dataframe=classification_report)})\n",
    "\n",
    "classification_report_artifact = wandb.Artifact(\n",
    "      f'classification_report_{model_name}', type='classification_report')\n",
    "\n",
    "with classification_report_artifact.new_file('classification_report.txt', mode='w') as f:\n",
    "    f.write(pprint.pformat(classification_report))\n",
    "\n",
    "wandb.run.log_artifact(classification_report_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminate WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark the end of the run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
