{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "211fdd0f",
   "metadata": {},
   "source": [
    "# Template for Accuracy Calculation for ChatGPT zero-shot and finetuned models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb92f2",
   "metadata": {},
   "source": [
    "This script provides some functionalities to calculate accuracy scores for the results of fine-tuned and zero-shot models. For zero-shot predictions, you need to re-map the output of the model to match with the labels of the original data to calculate accurate metrics, as the output might return full labels as opposed to abbreviations, or add filler words. If your task is not one of the tasks that we provide, you might need to add the mapping functions yourself by inspecting the output of your predictions (e.g. you can print all the unique combinations of output and original labels to see which categories have been created.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6a5935",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T11:57:32.100951389Z",
     "start_time": "2023-10-03T11:57:31.795989172Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "import pandas as pd \n",
    "from IPython.core.display import Markdown\n",
    "\n",
    "from dataload_utils import  load_dataset_task_prompt_mappings\n",
    "from label_utils import plot_count_and_normalized_confusion_matrix, task_to_display_labels, map_label_to_completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13518c4",
   "metadata": {},
   "source": [
    "### Define Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d5e19",
   "metadata": {},
   "source": [
    "Unlike with other models, ChatGPT returns the text clearly in all finetuned models, but in zero-shot models it has a tendency to return full labels, accompanied with an explanation. So the function belows converts zero-shot output into short labels by either returning the output or instances of mentions of full labels. If you find such instances in your finetuned data, you might want to run this on your finetuned models as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "273b79bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_zero_shotoutput(completion: str, task: int) -> str:\n",
    "    # Load mappings defined in Utils between short and full versions\n",
    "    mapping_dict = task_to_display_labels[task]\n",
    "    full_values = mapping_dict.get('full_name')\n",
    "    short_values = mapping_dict.get('short_name')\n",
    "\n",
    "    #assign null value if a text longer than 3 characters is returned that does not match one of the labels provided\n",
    "    val_out = \"NAN\"\n",
    "    #parse through completions to either return the short version of the full output (A, B, C etc.) or the original output\n",
    "    if \"Answer:\" in completion:\n",
    "        completion = completion.split(\"Answer:\")[1].strip()\n",
    "    #max length of short labels\n",
    "    if len(completion) > 3:\n",
    "        for full_val in full_values:\n",
    "            if full_val.lower() in completion.lower():\n",
    "                index_full = full_values.index(full_val)\n",
    "                val_out = short_values[index_full]\n",
    "                next\n",
    "    else:\n",
    "        val_out = completion\n",
    "    return val_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8072f8cb",
   "metadata": {},
   "source": [
    "### Set Up Arguments and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe645aa",
   "metadata": {},
   "source": [
    "Here, you define the values for the dataset that you are loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49667c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration variables\n",
    "\n",
    "#Name of the file to compute accuracy for\n",
    "prediction_file_name = \"data/predictions/chatgpt/ds_1_t_1_file.csv\"\n",
    "\n",
    "#Path to the dataset-task mappings file\n",
    "dataset_task_mappings_fp = os.path.normpath(os.path.join(module_dir, '..', 'dataset_task_mappings.csv'))\n",
    "\n",
    "# Type of task to run inference on\n",
    "task = 1  # Choices: [1,2,3,4,5,6]\n",
    "\n",
    "# Dataset to run inference on\n",
    "dataset = 1  # \n",
    "\n",
    "# Size of the sample to generate\n",
    "sample_size = '250'  # Enter 0 for zero-shot predictions\n",
    "\n",
    "# Zero-shot\n",
    "zero_shot = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063b9758",
   "metadata": {},
   "source": [
    "## Main Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eea4df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the file \n",
    "df = pd.read_csv(prediction_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed97bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the mapping file\n",
    "dataset_task_mappings_fp = pd.read_csv(dataset_task_mappings_fp)\n",
    "\n",
    "# load dataset mappings\n",
    "dataset_idx, dataset_task_mappings = load_dataset_task_prompt_mappings(\n",
    "    dataset_num=ds, task_num=task, dataset_task_mappings_fp=dataset_task_mappings_fp)\n",
    "label_column = dataset_task_mappings.loc[dataset_idx, \"label_column\"]\n",
    "labelset = dataset_task_mappings.loc[dataset_idx, \"labelset\"].split(\",\")\n",
    "labelset = [label.strip() for label in labelset]\n",
    "labelset_full_description = dataset_task_mappings.loc[dataset_idx, \"labelset_fullword\"].split(\"; \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c87aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run accuracy scores\n",
    "if zero_shot:\n",
    "    y_pred = df['prediction'].apply(lambda x : process_zero_shotoutput (x, task))\n",
    "    # Get ground truth in same format\n",
    "    y_true = df[label_column].map(lambda label: map_label_to_completion(\n",
    "        label=label, task_num=task, full_label=False))\n",
    "    #assert y_true.map(lambda pred: pred not in labelset).sum() == 0, 'Ground truth not in expected labelset'\n",
    "    display_labels = labelset\n",
    "else:\n",
    "    y_pred = df['prediction'].map(lambda label: map_label_to_completion(\n",
    "    label=label, task_num=task, full_label=True))\n",
    "    # Get ground truth in same format\n",
    "    y_true = df[label_column].map(lambda label: map_label_to_completion(\n",
    "        label=label, task_num=task, full_label=True))\n",
    "    #assert y_true.map(lambda pred: pred not in labelset_full_description).sum() == 0, 'Ground truth not in expected labelset'\n",
    "    display_labels = [label.upper() for label in labelset_full_description]\n",
    "    \n",
    "# Get accuracy\n",
    "#labels = labelset\n",
    "cm_plot, classification_report, metrics = plot_count_and_normalized_confusion_matrix(\n",
    "    y_true, y_pred, display_labels, display_labels, xticks_rotation='horizontal')\n",
    "\n",
    "# Get accuracy\n",
    "print({\n",
    "    'sample_size': sample_size,\n",
    "    'accuracy': metrics['accuracy'],\n",
    "    'f1-macro': metrics['f1'],\n",
    "    'precision': metrics['precision'],\n",
    "    'recall': metrics['recall']\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
