{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template for Zero-Shot Classification with OpenAI Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('c:\\\\Users\\\\maria\\\\OneDrive\\\\Documents\\\\GitHub\\\\DigDemLab\\\\LLM-Comparisons_repo\\\\OpenSource-LLM-Practitioner-Guide\\\\src\\\\finetune_chatGPT')\n",
    "sys.path.append('c:\\\\Users\\\\maria\\\\OneDrive\\\\Documents\\\\GitHub\\\\DigDemLab\\\\LLM-Comparisons_repo\\\\OpenSource-LLM-Practitioner-Guide\\\\src\\\\utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "import openai\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from utils import (\n",
    "    dataset_has_format_errors,\n",
    "    write_jsonl,\n",
    ")\n",
    "from dataload_utils import load_full_dataset,load_dataset_task_prompt_mappings\n",
    "from label_utils import plot_count_and_normalized_confusion_matrix, task_to_display_labels, map_label_to_completion\n",
    "\n",
    "module_dir = os.getcwd()\n",
    "\n",
    "# read API key\n",
    "with open('OpenAI_key.txt') as f:\n",
    "    openai.api_key = f.readlines()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Arguments and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block, you are required to set up several key parameters that will define the behavior and environment of your fine-tuning process:\n",
    "\n",
    "1. **WandB Project Name (`WANDB_PROJECT_NAME`)**: This is the name of the project in Weights & Biases (WandB) where your training run will be logged. WandB is a tool that helps track experiments, visualize data, and share insights. By setting the project name here, you ensure that all the metrics, outputs, and logs from your training process are organized under a single project for easy access and comparison. Specify a meaningful name that reflects the nature of your training session or experiment. If you leave the argument empty, the project will not be tracked on WandB.\n",
    "\n",
    "2. **Model Name (`MODEL_NAME`)**: Here, you select the specific model from OpenAI's suite that you wish to fine-tune. The model name, such as `'gpt-3.5-turbo-0613'`, refers to a particular configuration and version of the model. This selection dictates the starting point of your fine-tuning process, leveraging the pre-trained weights and architecture of the specified model. Ensure that the model name corresponds to an existing and available model in OpenAI's library. You can find more models here:  https://platform.openai.com/docs/models \n",
    "\n",
    "3. **Completion Retries (`COMPLETION_RETRIES`)**: This parameter defines the number of retry attempts for generating a completion in case the initial attempts fail. When interacting with the model, especially in a fine-tuning context, certain queries may not succeed on the first try due to various reasons (e.g., network issues, API errors). This setting provides resilience, allowing the process to attempt the generation multiple times before considering it a failure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specs WandB and Which Model you want to fine-tune\n",
    "PROJECT_NAME = \"chatGPT_template_1\"\n",
    "WANDB_PROJECT_NAME = \"chatGPT_template_1\"\n",
    "MODEL_NAME = 'gpt-3.5-turbo-1106'\n",
    "COMPLETION_RETRIES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code block, you are required to set up various configuration variables that will dictate how the inference processes are executed. These variables are crucial as they define the nature of the task, the data, and the specific behaviors during the model's training and evaluation.\n",
    "\n",
    "1. **Task (`task`)**: Specify the type of task you want to run inference on. The task is represented by an integer, with each number corresponding to a different type of task (e.g., 1, 2, 3, etc.). You must select from the predefined choices, which are typically mapped to specific NLP tasks or scenarios.\n",
    "\n",
    "2. **Dataset (`dataset`)**: Choose the dataset on which you want to run inference. Like tasks, datasets are identified by integers, and each number corresponds to a different dataset. Ensure that the dataset selected is relevant to the task at hand.\n",
    "\n",
    "3. **Output Directory (`output_dir`)**: Define the path to the directory where you want to store the generated samples. This is where the output of your training and inference processes will be saved.\n",
    "\n",
    "4. **Random Seed (`seed`)**: Setting a random seed ensures that the results are reproducible. By using the same seed, you can achieve the same outcomes on repeated runs under identical conditions.\n",
    "\n",
    "5. **Data Directory (`data_dir`)**: Specify the path to the directory containing the datasets you plan to use for training and evaluation.\n",
    "\n",
    "6. **Label Usage (`not_use_full_labels`)**: This boolean variable determines whether to use the full label descriptions or abbreviated labels during training and inference. Setting it to `False` means full labels will be used.\n",
    "\n",
    "7. **Dataset-Task Mappings File Path (`dataset_task_mappings_fp`)**: Define the path to the file containing mappings between datasets and tasks. This file is crucial for ensuring the correct dataset is used for the specified task.\n",
    "\n",
    "8. **Rewrite DataFrame (`rewrite_df_in_openai`)**: A boolean that dictates whether to rewrite the dataframe in OpenAI format. If set to `True`, the data will be reformatted according to OpenAI's expected input structure during the execution.\n",
    "\n",
    "9. **Number of Epochs (`n_epochs`)**: Specify the number of epochs for training the model. An epoch refers to one complete pass through the entire training dataset.\n",
    "\n",
    "10. **Run Name (`run_name`)**: Give a unique name to your run, which will help you identify it later, especially when tracking multiple experiments or runs.\n",
    "\n",
    "11. **Temperature (`temp`)**: Set the temperature for text generation. Temperature controls the randomness of the output; a lower temperature results in less random completions.\n",
    "\n",
    "12. **Few-shot (`few_shot`)**: A boolean indicating whether to use a few-shot learning approach. When set to `True`, the model will be fine-tuned with only a few examples.\n",
    "\n",
    "13. **System-User Prompt Division (`system_user_division`)**: This integer defines the separation between system and user prompts, which is critical for structuring the input data correctly for the model.\n",
    "\n",
    "**Customizing for Your Own Tasks:**\n",
    "If you plan to run a custom task or use a dataset that is not predefined, you will need to make modifications to the `utils_src` file. This file contains all mappings for different datasets and tasks. Adding your custom task or dataset involves defining the new task or dataset number and specifying its characteristics and mappings in the `utils_src` file. This ensures that your custom task or dataset integrates seamlessly with the existing framework for training and inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Variables\n",
    "\n",
    "# Number of task to run inference on\n",
    "task = 1  # As defined in dataset_task_mappings.csv 1-6 or more if you add them in label_utils\n",
    "\n",
    "# Number of dataset to run inference on\n",
    "dataset = 1  # s defined in dataset_task_mappings.csv\n",
    "\n",
    "# Path to the directory containing the datasets\n",
    "data_dir = '../../data'\n",
    "\n",
    "#Name of dataset to run inference on\n",
    "#NOTE: we recommend keeping the names of the training and evaluation sets in the way we provided.\n",
    "eval_set_name = f\"ds_{dataset}__task_{task}_eval_set_full\"\n",
    "\n",
    "# Path to the data directory\n",
    "output_dir = '../../data'\n",
    "\n",
    "# Random seed to use\n",
    "seed = 2019\n",
    "\n",
    "# Path to the directory containing the datasets\n",
    "data_dir = '../../data'\n",
    "\n",
    "# Whether to use the full label\n",
    "use_full_labels = True\n",
    "\n",
    "# Path to the dataset-task mappings file\n",
    "dataset_task_mappings_fp = os.path.normpath(os.path.join(module_dir, '..', '..', 'dataset_task_mappings.csv'))\n",
    "\n",
    "# Whether to rewrite the dataframe in OpenAI format\n",
    "rewrite_df_in_openai = True\n",
    "\n",
    "# Number of epochs to train the model\n",
    "n_epochs = 3\n",
    "\n",
    "# Name of the run - if not empty make sure the run identifies which dataset and task is used\n",
    "run_name = ''\n",
    "\n",
    "# Temperature to use when generating text\n",
    "temp = 0.0\n",
    "\n",
    "# Fewshot\n",
    "few_shot = False\n",
    "\n",
    "# Separation between system and user prompt\n",
    "system_user_prompt_division_line = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here how the run_name is automatically produced and change if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_run_name = run_name if run_name != '' else f'ds_{dataset}_t_{int(task)}_sample_0__fl_{str(use_full_labels)}_temp_{temp}_{MODEL_NAME}'\n",
    "if few_shot:\n",
    "    project_run_name += \"few_shot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_1_t_1_sample_0__fl_True_temp_0.0_gpt-3.5-turbo-1106\n"
     ]
    }
   ],
   "source": [
    "print(project_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_example(system_prompt, user_prompt_format, user_prompt_text, completion):\n",
    "    return {'messages': [\n",
    "        {'role': 'system',\n",
    "         'content': system_prompt},\n",
    "\n",
    "        {'role': 'user',\n",
    "         'content': user_prompt_format.format(text=user_prompt_text)},\n",
    "\n",
    "        {'role': 'assistant',\n",
    "         'content': completion}\n",
    "    ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: can any of these be moved to utils?\n",
    "def upload_datasets_to_openai(output_dir, not_use_full_labels, rewrite_df_in_openai, datasets):\n",
    "\n",
    "    uploaded_files = client.files.list()\n",
    "    #store IDs created in this session for easier reference\n",
    "    datasets_open_ai_metadata = list()\n",
    "\n",
    "    df_id_metadata = pd.DataFrame(columns = [\"df_name\", \"file_id\"]) if not os.path.exists('datasets_open_ai_metadata.csv') \\\n",
    "        else pd.read_csv('datasets_open_ai_metadata.csv')\n",
    "\n",
    "    for df_name, df in datasets.items():\n",
    "        #file to send\n",
    "        df_filename = f'{PROJECT_NAME}_{df_name}'\n",
    "        df_jsonl_filename = os.path.join(output_dir, 'temp', df_filename+'.jsonl')\n",
    "        write_jsonl(data_list=df['openai_instance_format'].tolist(), filename=df_jsonl_filename)\n",
    "\n",
    "        if not_use_full_labels:\n",
    "            df_filename += '_single_letter_labels'\n",
    "\n",
    "        #check how many files are already\n",
    "        matches_openai = [d.id for d in uploaded_files.data if d.filename == df_filename]\n",
    "\n",
    "        #check if there are matches in the dataframe\n",
    "        matches_stored_data = df_id_metadata.loc[df_id_metadata['df_name'] == df_filename, 'file_id'].values if df_filename in df_id_metadata['df_name'].values else []\n",
    "\n",
    "        #skip uploading only if the ID in the local file corresponds to the ID's in the Organization account\n",
    "        if not rewrite_df_in_openai and len(matches_stored_data) > 0 and len(matches_openai) > 0:\n",
    "                if matches_stored_data[-1] in matches_openai:\n",
    "                    print(f\"Dataset {df_name} has already uploaded to OpenAI during this project.\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"Dataset of this name has already been uploaded to OpenAI outside of this project under ID {matches_openai[0]}. Please add this ID to metadata manually if you don't want to re-upload this file.\")\n",
    "                continue\n",
    "        \n",
    "        #otherwise upload the files to OpenAI\n",
    "        print(f\"Uploading {df_filename} to OpenAI\")\n",
    "        df_response = client.files.create(\n",
    "            file=open(df_jsonl_filename, \"rb\"), purpose=\"fine-tune\"\n",
    "        )\n",
    "        df_file_id = df_response.id\n",
    "\n",
    "        # Wait until the file is processed\n",
    "        while True:\n",
    "            file = client.files.retrieve(df_file_id)\n",
    "            if file.status == \"processed\":\n",
    "                break\n",
    "            time.sleep(15)\n",
    "        datasets_open_ai_metadata.append({'df_name': df_filename, 'file_id': df_file_id})\n",
    "\n",
    "    #Store IDs of file uploaded in this session for later check\n",
    "    df_id_metadata = pd.concat([df_id_metadata, pd.DataFrame(datasets_open_ai_metadata)])\n",
    "    df_id_metadata.to_csv('datasets_open_ai_metadata.csv', index=False)\n",
    "\n",
    "    return df_id_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaria-korobeynikova\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\maria\\OneDrive\\Documents\\GitHub\\DigDemLab\\LLM-Comparisons_repo\\OpenSource-LLM-Practitioner-Guide\\src\\finetune_chatGPT\\wandb\\run-20240415_225218-8gwrzo8q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/maria-korobeynikova/chatGPT_template_1/runs/8gwrzo8q' target=\"_blank\">gpt-3.5-turbo-1106_ds_1_t_1_sample_0__fl_True_temp_0.0</a></strong> to <a href='https://wandb.ai/maria-korobeynikova/chatGPT_template_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/maria-korobeynikova/chatGPT_template_1' target=\"_blank\">https://wandb.ai/maria-korobeynikova/chatGPT_template_1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/maria-korobeynikova/chatGPT_template_1/runs/8gwrzo8q' target=\"_blank\">https://wandb.ai/maria-korobeynikova/chatGPT_template_1/runs/8gwrzo8q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/maria-korobeynikova/chatGPT_template_1/runs/8gwrzo8q?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2cbd0345900>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Weights and Biases run\n",
    "if WANDB_PROJECT_NAME != \"\":\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=WANDB_PROJECT_NAME,\n",
    "        name=project_run_name,\n",
    "        # track hyperparameters and run metadata\n",
    "        config = {\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"dataset\": dataset,\n",
    "            \"task\": task,\n",
    "            \"epochs\": n_epochs,\n",
    "            \"temp\": temp\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_idx, dataset_task_mappings = load_dataset_task_prompt_mappings(\n",
    "    dataset_num=dataset, task_num=task, dataset_task_mappings_fp=dataset_task_mappings_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information specific to the dataset\n",
    "label_column = dataset_task_mappings.loc[dataset_idx, \"label_column\"]\n",
    "if few_shot:\n",
    "    prompt = dataset_task_mappings.loc[dataset_idx, 'few_shot_prompt']\n",
    "else:\n",
    "    prompt = dataset_task_mappings.loc[dataset_idx, 'zero_shot_prompt']\n",
    "\n",
    "labelset = dataset_task_mappings.loc[dataset_idx, \"labelset_fullword\" if use_full_labels else \"labelset\"].split(\";\")\n",
    "labelset = [label.strip() for label in labelset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is where the user prompt starts:\n",
      "Now, is the following tweet RELEVANT or IRRELEVANT to content moderation?\n",
      "\n",
      "{text}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Artifact prompts>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = ('\\n'.join(prompt.split('\\n')[:-system_user_prompt_division_line])).strip()\n",
    "user_prompt_format = ('\\n'.join(prompt.split('\\n')[-system_user_prompt_division_line:])).strip()\n",
    "print(\"This is where the user prompt starts:\")\n",
    "print(user_prompt_format)\n",
    "\n",
    "# Log the system prompt and user_prompt_format as files in wandb\n",
    "prompts_artifact = wandb.Artifact('prompts', type='prompts')\n",
    "with prompts_artifact.new_file('system_prompt.txt', mode='w', encoding=\"utf-8\") as f:\n",
    "    f.write(system_prompt)\n",
    "with prompts_artifact.new_file('user_prompt_format.txt', mode='w', encoding=\"utf-8\") as f:\n",
    "    f.write(user_prompt_format)\n",
    "wandb.run.log_artifact(prompts_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_full_dataset(data_dir, dataset, task)\n",
    "eval_df = datasets[eval_set_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_output_dir = os.path.join(\n",
    "    output_dir, 'preprocessed', 'full_name_labels' if use_full_labels else 'single_letter_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for errors in the prediction set: \n",
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "#make an OpenAI type datafile to run predictions on\n",
    "eval_df['completion_label'] = eval_df[label_column].map(lambda label: map_label_to_completion(label=label, task_num=task,\n",
    "                                        full_label=use_full_labels)\n",
    ")\n",
    "eval_df['openai_instance_format'] = eval_df.apply(lambda row: create_training_example(\n",
    "    system_prompt=system_prompt, user_prompt_format=user_prompt_format,\n",
    "    user_prompt_text=row['text'],completion=row['completion_label']),axis=1)\n",
    "\n",
    "eval_df['openai_instance_without_completion'] = eval_df['openai_instance_format'].map(lambda x: x['messages'][:-1])\n",
    "\n",
    "#check for any errors\n",
    "print(f'Check for errors in the prediction set: ')\n",
    "assert not dataset_has_format_errors(eval_df['openai_instance_format'].tolist()), f\"Errors found\"\n",
    "\n",
    "#export pre-processed output (optional)\n",
    "os.makedirs(preprocessed_output_dir, exist_ok= True)\n",
    "eval_df.to_csv(os.path.join(preprocessed_output_dir,eval_set_name + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chatGPT_template_1_ds_1__task_1_eval_set_full to OpenAI\n"
     ]
    }
   ],
   "source": [
    "# Create jsonl file and upload to OpenAI\n",
    "df_id_metadata = upload_datasets_to_openai(output_dir, not use_full_labels, rewrite_df_in_openai, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1935 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting predictions. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1935 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 8\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     13\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\envs\\llm_finetune_cp\\lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[1;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError getting predictions. Retrying...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     num_retries \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_retries \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m COMPLETION_RETRIES:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for messages in tqdm(eval_df['openai_instance_without_completion'].tolist()):\n",
    "    # Retry the completion at least COMPLETION_RETRIES times\n",
    "    num_retries = 0\n",
    "    response = None\n",
    "    while num_retries < COMPLETION_RETRIES and response is None:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages,\n",
    "            temperature=temp,\n",
    "            n=1\n",
    "        )\n",
    "        except Exception as e:\n",
    "            print('Error getting predictions. Retrying...')\n",
    "            time.sleep(5)\n",
    "            num_retries += 1\n",
    "            if num_retries >= COMPLETION_RETRIES:\n",
    "                print('Maximum amount of retires reached')\n",
    "                raise e\n",
    "    predictions.append(response['choices'][0]['message']['content'])\n",
    "\n",
    "# Add predictions to df\n",
    "eval_df['prediction'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store output\n",
    "predictions_output_dir = os.path.join(output_dir, 'predictions_chatGPT')\n",
    "os.makedirs(predictions_output_dir, exist_ok=True)\n",
    "datasets[eval_set_name].to_csv(os.path.join(predictions_output_dir, f\"{project_run_name}.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run preliminary accuracy for WandB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This module just compares the accuracy between the label and the prediction - espcially in zero-shot predictions, the output is likely to not be in the same format as the original label. To get the real accuracy scores we recommend using the 02-measure_performance scripts which extracts the labels from prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get performance metrics--\n",
    "y_true = eval_df['completion_label']\n",
    "y_pred = eval_df['prediction']\n",
    "\n",
    "label_type = 'full_name' if use_full_labels else 'short_name'\n",
    "display_labels = task_to_display_labels[task][label_type]\n",
    "labels = display_labels\n",
    "\n",
    "cm_plot, classification_report, metrics = plot_count_and_normalized_confusion_matrix(\n",
    "    y_true, y_pred, display_labels, labels, xticks_rotation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log metrics\n",
    "if WANDB_PROJECT_NAME != \"\":\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        wandb.log({metric_name: metric_value})\n",
    "\n",
    "    # Log the confusion matrix matplotlib figure\n",
    "    wandb.log({'confusion_matrix': wandb.Image(cm_plot)})\n",
    "\n",
    "    # Log the classification report as an artifact\n",
    "    classification_report = (pd.DataFrame({k: v for k, v in classification_report.items() if k != 'accuracy'})\n",
    "                                .transpose().reset_index())\n",
    "    wandb.log({'classification_report': wandb.Table(\n",
    "        dataframe=classification_report)})\n",
    "\n",
    "    classification_report_artifact = wandb.Artifact(\n",
    "        f'classification_report_{model_name}', type='classification_report')\n",
    "\n",
    "    with classification_report_artifact.new_file('classification_report.txt', mode='w') as f:\n",
    "        f.write(pprint.pformat(classification_report))\n",
    "\n",
    "    wandb.run.log_artifact(classification_report_artifact)\n",
    "\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
