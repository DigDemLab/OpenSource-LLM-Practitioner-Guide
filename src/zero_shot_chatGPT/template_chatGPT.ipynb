{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template for Zero-Shot Classification with OpenAI Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import time\n",
    "from ast import literal_eval\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "from utils import (\n",
    "    dataset_has_format_errors,\n",
    "    write_jsonl,\n",
    ")\n",
    "from dataload_utils import load_full_dataset,load_dataset_task_prompt_mappings\n",
    "from label_utils import task_num_to_task_name, dataset_num_to_dataset_name, plot_count_and_normalized_confusion_matrix, task_to_display_labels, map_label_to_completion\n",
    "\n",
    "module_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# read API key\n",
    "with open('src/OpenAI_key.txt') as f:\n",
    "    openai.api_key = f.readlines()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Arguments and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block, you are required to set up several key parameters that will define the behavior and environment of your fine-tuning process:\n",
    "\n",
    "1. **WandB Project Name (`WANDB_PROJECT_NAME`)**: This is the name of the project in Weights & Biases (WandB) where your training run will be logged. WandB is a tool that helps track experiments, visualize data, and share insights. By setting the project name here, you ensure that all the metrics, outputs, and logs from your training process are organized under a single project for easy access and comparison. Specify a meaningful name that reflects the nature of your training session or experiment.\n",
    "\n",
    "2. **Model Name (`MODEL_NAME`)**: Here, you select the specific model from OpenAI's suite that you wish to fine-tune. The model name, such as `'gpt-3.5-turbo-0613'`, refers to a particular configuration and version of the model. This selection dictates the starting point of your fine-tuning process, leveraging the pre-trained weights and architecture of the specified model. Ensure that the model name corresponds to an existing and available model in OpenAI's library. You can find more models here:  https://platform.openai.com/docs/models \n",
    "\n",
    "3. **Completion Retries (`COMPLETION_RETRIES`)**: This parameter defines the number of retry attempts for generating a completion in case the initial attempts fail. When interacting with the model, especially in a fine-tuning context, certain queries may not succeed on the first try due to various reasons (e.g., network issues, API errors). This setting provides resilience, allowing the process to attempt the generation multiple times before considering it a failure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specs WandB and Which Model you want to fine-tune\n",
    "WANDB_PROJECT_NAME = \"chatGPT_template_1\"\n",
    "MODEL_NAME = 'gpt-3.5-turbo-0613'\n",
    "COMPLETION_RETRIES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code block, you are required to set up various configuration variables that will dictate how the inference processes are executed. These variables are crucial as they define the nature of the task, the data, and the specific behaviors during the model's training and evaluation.\n",
    "\n",
    "1. **Task (`task`)**: Specify the type of task you want to run inference on. The task is represented by an integer, with each number corresponding to a different type of task (e.g., 1, 2, 3, etc.). You must select from the predefined choices, which are typically mapped to specific NLP tasks or scenarios.\n",
    "\n",
    "2. **Dataset (`dataset`)**: Choose the dataset on which you want to run inference. Like tasks, datasets are identified by integers, and each number corresponds to a different dataset. Ensure that the dataset selected is relevant to the task at hand.\n",
    "\n",
    "3. **Output Directory (`output_dir`)**: Define the path to the directory where you want to store the generated samples. This is where the output of your training and inference processes will be saved.\n",
    "\n",
    "4. **Random Seed (`seed`)**: Setting a random seed ensures that the results are reproducible. By using the same seed, you can achieve the same outcomes on repeated runs under identical conditions.\n",
    "\n",
    "5. **Data Directory (`data_dir`)**: Specify the path to the directory containing the datasets you plan to use for training and evaluation.\n",
    "\n",
    "6. **Label Usage (`not_use_full_labels`)**: This boolean variable determines whether to use the full label descriptions or abbreviated labels during training and inference. Setting it to `False` means full labels will be used.\n",
    "\n",
    "7. **Dataset-Task Mappings File Path (`dataset_task_mappings_fp`)**: Define the path to the file containing mappings between datasets and tasks. This file is crucial for ensuring the correct dataset is used for the specified task.\n",
    "\n",
    "8. **Rewrite DataFrame (`rewrite_df_in_openai`)**: A boolean that dictates whether to rewrite the dataframe in OpenAI format. If set to `True`, the data will be reformatted according to OpenAI's expected input structure during the execution.\n",
    "\n",
    "9. **Number of Epochs (`n_epochs`)**: Specify the number of epochs for training the model. An epoch refers to one complete pass through the entire training dataset.\n",
    "\n",
    "10. **Run Name (`run_name`)**: Give a unique name to your run, which will help you identify it later, especially when tracking multiple experiments or runs.\n",
    "\n",
    "11. **Temperature (`temp`)**: Set the temperature for text generation. Temperature controls the randomness of the output; a lower temperature results in less random completions.\n",
    "\n",
    "12. **Few-shot (`few_shot`)**: A boolean indicating whether to use a few-shot learning approach. When set to `True`, the model will be fine-tuned with only a few examples.\n",
    "\n",
    "13. **System-User Prompt Division (`system_user_division`)**: This integer defines the separation between system and user prompts, which is critical for structuring the input data correctly for the model.\n",
    "\n",
    "**Customizing for Your Own Tasks:**\n",
    "If you plan to run a custom task or use a dataset that is not predefined, you will need to make modifications to the `utils_src` file. This file contains all mappings for different datasets and tasks. Adding your custom task or dataset involves defining the new task or dataset number and specifying its characteristics and mappings in the `utils_src` file. This ensures that your custom task or dataset integrates seamlessly with the existing framework for training and inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Variables\n",
    "\n",
    "# Type of task to run inference on\n",
    "task = 1  # Choices: [1,2,3,4,5,6]\n",
    "\n",
    "# Dataset to run inference on\n",
    "dataset = 1  # Choices: [1, 2, 3, 4]\n",
    "\n",
    "# Path to the directory to store the generated samples\n",
    "output_dir = '../../data'\n",
    "\n",
    "# Random seed to use\n",
    "seed = 2019\n",
    "\n",
    "# Path to the directory containing the datasets\n",
    "data_dir = '../../data'\n",
    "\n",
    "# Whether to use the full label\n",
    "not_use_full_labels = False\n",
    "\n",
    "# Path to the dataset-task mappings file\n",
    "dataset_task_mappings_fp = os.path.normpath(os.path.join(module_dir, '..', 'dataset_task_mappings.csv'))\n",
    "\n",
    "# Whether to rewrite the dataframe in OpenAI format\n",
    "rewrite_df_in_openai = True\n",
    "\n",
    "# Number of epochs to train the model\n",
    "n_epochs = 3\n",
    "\n",
    "# Name of the run - if not empty make sure the run identifies which dataset and task is used\n",
    "run_name = ''\n",
    "\n",
    "# Temperature to use when generating text\n",
    "temp = 0.0\n",
    "\n",
    "# Fewshot\n",
    "few_shot = False\n",
    "\n",
    "# Separation between system and user prompt\n",
    "system_user_prompt_division_line = 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here how the run_name is automatically produced and change if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_run_name = run_name if run_name != '' else f'{MODEL_NAME}_ds_{dataset}_t_{int(task)}_sample_0__sl_{str(not_use_full_labels)}_temp_{temp}'\n",
    "if few_shot:\n",
    "    project_run_name += \"few_shot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(project_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Unitily Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_example(system_prompt, user_prompt_format, user_prompt_text, completion):\n",
    "    return {'messages': [\n",
    "        {'role': 'system',\n",
    "         'content': system_prompt},\n",
    "\n",
    "        {'role': 'user',\n",
    "         'content': user_prompt_format.format(text=user_prompt_text)},\n",
    "\n",
    "        {'role': 'assistant',\n",
    "         'content': completion}\n",
    "    ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: can any of these be moved to utils?\n",
    "def upload_datasets_to_openai(output_dir, not_use_full_labels, rewrite_df_in_openai, datasets):\n",
    "\n",
    "    uploaded_files = client.files.list()\n",
    "    #store IDs created in this session for easier reference\n",
    "    datasets_open_ai_metadata = list()\n",
    "\n",
    "    df_id_metadata = pd.DataFrame(columns = [\"df_name\", \"file_id\"]) if not os.path.exists('datasets_open_ai_metadata.csv') \\\n",
    "        else pd.read_csv('datasets_open_ai_metadata.csv')\n",
    "\n",
    "    for df_name, df in datasets.items():\n",
    "        #file to send\n",
    "        df_filename = f'{WANDB_PROJECT_NAME}_{df_name}'\n",
    "        df_jsonl_filename = os.path.join(output_dir, 'temp', df_filename+'.jsonl')\n",
    "        write_jsonl(data_list=df['openai_instance_format'].tolist(), filename=df_jsonl_filename)\n",
    "\n",
    "        if not_use_full_labels:\n",
    "            df_filename += '_single_letter_labels'\n",
    "\n",
    "        #check how many files are already\n",
    "        matches_openai = [d.id for d in uploaded_files.data if d.filename == df_filename]\n",
    "\n",
    "        #check if there are matches in the dataframe\n",
    "        matches_stored_data = df_id_metadata.loc[df_id_metadata['df_name'] == df_filename, 'file_id'].values if df_filename in df_id_metadata['df_name'].values else []\n",
    "\n",
    "        #skip uploading only if the ID in the local file corresponds to the ID's in the Organization account\n",
    "        if not rewrite_df_in_openai and len(matches_stored_data) > 0 and len(matches_openai) > 0:\n",
    "                if matches_stored_data[-1] in matches_openai:\n",
    "                    print(f\"Dataset {df_name} has already uploaded to OpenAI during this project.\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"Dataset of this name has already been uploaded to OpenAI outside of this project under ID {matches_openai[0]}. Please add this ID to metadata manually if you don't want to re-upload this file.\")\n",
    "                continue\n",
    "        \n",
    "        #otherwise upload the files to OpenAI\n",
    "        print(f\"Uploading {df_filename} to OpenAI\")\n",
    "        df_response = client.files.create(\n",
    "            file=open(df_jsonl_filename, \"rb\"), purpose=\"fine-tune\"\n",
    "        )\n",
    "        df_file_id = df_response.id\n",
    "\n",
    "        # Wait until the file is processed\n",
    "        while True:\n",
    "            file = client.files.retrieve(df_file_id)\n",
    "            if file.status == \"processed\":\n",
    "                break\n",
    "            time.sleep(15)\n",
    "        datasets_open_ai_metadata.append({'df_name': df_filename, 'file_id': df_file_id})\n",
    "\n",
    "    #Store IDs of file uploaded in this session for later check\n",
    "    df_id_metadata = pd.concat([df_id_metadata, pd.DataFrame(datasets_open_ai_metadata)])\n",
    "    df_id_metadata.to_csv('datasets_open_ai_metadata.csv', index=False)\n",
    "\n",
    "    return df_id_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Weights and Biases run\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=WANDB_PROJECT_NAME,\n",
    "    name=project_run_name,\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"dataset\": dataset_num_to_dataset_name[int(dataset)],\n",
    "        \"task\": task_num_to_task_name[int(task)],\n",
    "        \"epochs\": n_epochs,\n",
    "        \"temp\": temp\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_idx, dataset_task_mappings = load_dataset_task_prompt_mappings(\n",
    "    dataset_num=dataset, task_num=task, dataset_task_mappings_fp=dataset_task_mappings_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information specific to the dataset\n",
    "label_column = dataset_task_mappings.loc[dataset_idx, \"label_column\"]\n",
    "if few_shot:\n",
    "    prompt = dataset_task_mappings.loc[dataset_idx, 'few_shot_prompt']\n",
    "else:\n",
    "    prompt = dataset_task_mappings.loc[dataset_idx, 'zero_shot_prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt = ('\\n'.join(prompt.split('\\n')[:-system_user_prompt_division_line])).strip()\n",
    "user_prompt_format = ('\\n'.join(prompt.split('\\n')[-system_user_prompt_division_line:])).strip()\n",
    "print(user_prompt_format)\n",
    "\n",
    "# Log the system prompt and user_prompt_format as files in wandb\n",
    "prompts_artifact = wandb.Artifact('prompts', type='prompts')\n",
    "with prompts_artifact.new_file('system_prompt.txt', mode='w', encoding=\"utf-8\") as f:\n",
    "    f.write(system_prompt)\n",
    "with prompts_artifact.new_file('user_prompt_format.txt', mode='w', encoding=\"utf-8\") as f:\n",
    "    f.write(user_prompt_format)\n",
    "wandb.run.log_artifact(prompts_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_full_dataset(data_dir=data_dir, dataset_num=dataset, task_num=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set_name = f'ds_{dataset}__task_{task}_eval_set_full'\n",
    "eval_df = datasets[eval_set_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_output_dir = os.path.join(\n",
    "    output_dir, 'preprocessed', 'full_name_labels' if not not_use_full_labels else 'single_letter_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in datasets.items():\n",
    "        print(df_name)\n",
    "        print(df.head())\n",
    "        df['completion_label'] = df[label_column].map(\n",
    "            lambda label: map_label_to_completion(label=label, task_num=task,\n",
    "                                                  full_label=not not_use_full_labels)\n",
    "        )\n",
    "        df['openai_instance_format'] = df.apply(\n",
    "            lambda row: create_training_example(\n",
    "                system_prompt=system_prompt, user_prompt_format=user_prompt_format,\n",
    "                user_prompt_text=row['text'],\n",
    "                completion=row['completion_label']\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        df['openai_instance_without_completion'] = df['openai_instance_format'].map(lambda x: x['messages'][:-1])\n",
    "\n",
    "        print(f'Check for errors {df_name} set: ')\n",
    "        assert not dataset_has_format_errors(df['openai_instance_format'].tolist()), f\"Errors found in {df_name}\"\n",
    "        os.makedirs(preprocessed_output_dir, exist_ok= True)\n",
    "        df.to_csv(os.path.join(preprocessed_output_dir, df_name + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create jsonl file and upload to OpenAI\n",
    "df_id_metadata = upload_datasets_to_openai(output_dir, not_use_full_labels, rewrite_df_in_openai, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_name = f'ds_{dataset}__task_{task}_train_set'\n",
    "model_name = (train_set_name.replace('__', '_')\n",
    "                .replace('train_set', 'trn')\n",
    "                .replace('task', 't')\n",
    "                .replace('_single_letter_labels', '_sl'))\n",
    "\n",
    "if few_shot:\n",
    "    model_name += \"_few_shot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the evaluation set and store the predictions\n",
    "print(\"\\n\" + \"#\" * 50)\n",
    "print(\"Getting predictions on the evaluation set\")\n",
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for messages in tqdm(eval_df['openai_instance_without_completion'].tolist()):\n",
    "    # Retry the completion at least COMPLETION_RETRIES times\n",
    "    num_retries = 0\n",
    "    response = None\n",
    "    while num_retries < COMPLETION_RETRIES and response is None:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages,\n",
    "            temperature=temp,\n",
    "            n=1\n",
    "        )\n",
    "        except Exception as e:\n",
    "            print('Error getting predictions. Retrying...')\n",
    "            time.sleep(5)\n",
    "            num_retries += 1\n",
    "            if num_retries >= COMPLETION_RETRIES:\n",
    "                print('Maximum amount of retires reached')\n",
    "                raise e\n",
    "    predictions.append(response['choices'][0]['message']['content'])\n",
    "\n",
    "# Add predictions to df\n",
    "eval_df['prediction'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store output\n",
    "predictions_output_dir = os.path.join(output_dir, 'predictions',\n",
    "                                        f'dataset_{dataset}_task_{task}')\n",
    "os.makedirs(predictions_output_dir, exist_ok=True)\n",
    "#edited after running\n",
    "\n",
    "datasets[eval_set_name].to_csv(\n",
    "    os.path.join(predictions_output_dir, f\"{MODEL_NAME}--{run_name}.csv\"),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get performance metrics--\n",
    "y_true = eval_df['completion_label']\n",
    "y_pred = eval_df['prediction']\n",
    "\n",
    "label_type = 'full_name' if not not_use_full_labels else 'short_name'\n",
    "display_labels = task_to_display_labels[task][label_type]\n",
    "labels = display_labels\n",
    "\n",
    "cm_plot, classification_report, metrics = plot_count_and_normalized_confusion_matrix(\n",
    "    y_true, y_pred, display_labels, labels, xticks_rotation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log metrics\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    wandb.log({metric_name: metric_value})\n",
    "\n",
    "# Log the confusion matrix matplotlib figure\n",
    "wandb.log({'confusion_matrix': wandb.Image(cm_plot)})\n",
    "\n",
    "# Log the classification report as an artifact\n",
    "classification_report = (pd.DataFrame({k: v for k, v in classification_report.items() if k != 'accuracy'})\n",
    "                            .transpose().reset_index())\n",
    "wandb.log({'classification_report': wandb.Table(\n",
    "    dataframe=classification_report)})\n",
    "\n",
    "classification_report_artifact = wandb.Artifact(\n",
    "    f'classification_report_{model_name}', type='classification_report')\n",
    "\n",
    "with classification_report_artifact.new_file('classification_report.txt', mode='w') as f:\n",
    "    f.write(pprint.pformat(classification_report))\n",
    "\n",
    "wandb.run.log_artifact(classification_report_artifact)\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_finetune_cp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
