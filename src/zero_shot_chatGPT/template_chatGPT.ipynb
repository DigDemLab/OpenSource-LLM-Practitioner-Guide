{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template for Zero-Shot Classification with OpenAI Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pprint\n",
    "import time\n",
    "import argparse\n",
    "from ast import literal_eval\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "from utils import (\n",
    "    dataset_has_format_errors,\n",
    "    write_jsonl,\n",
    ")\n",
    "from utils_src import task_num_to_task_name, dataset_num_to_dataset_name, plot_count_and_normalized_confusion_matrix, \\\n",
    "    task_to_display_labels, load_dataset_task_prompt_mappings\n",
    "\n",
    "module_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# read API key\n",
    "with open('src/OpenAI_key.txt') as f:\n",
    "    openai.api_key = f.readlines()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Arguments and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specs WandB and Which Model you want to fine-tune\n",
    "WANDB_PROJECT_NAME = \"chatGPT_template_1\"\n",
    "MODEL_NAME = 'gpt-3.5-turbo-0613'\n",
    "COMPLETION_RETRIES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Variables\n",
    "\n",
    "# Type of task to run inference on\n",
    "task = 1  # Choices: [1,2,3,4,5,6]\n",
    "\n",
    "# Dataset to run inference on\n",
    "dataset = 1  # Choices: [1, 2, 3, 4]\n",
    "\n",
    "# Path to the directory to store the generated samples\n",
    "output_dir = '../../data'\n",
    "\n",
    "# Random seed to use\n",
    "seed = 2019\n",
    "\n",
    "# Path to the directory containing the datasets\n",
    "data_dir = '../../data'\n",
    "\n",
    "# Whether to use the full label\n",
    "not_use_full_labels = False\n",
    "\n",
    "# Path to the dataset-task mappings file\n",
    "dataset_task_mappings_fp = os.path.normpath(os.path.join(module_dir, '..', 'dataset_task_mappings.csv'))\n",
    "\n",
    "# Whether to rewrite the dataframe in OpenAI format\n",
    "rewrite_df_in_openai = True\n",
    "\n",
    "# Number of epochs to train the model\n",
    "n_epochs = 3\n",
    "\n",
    "# Name of the run\n",
    "run_name = 'zeroshot_chatGPT_3.5_template'\n",
    "\n",
    "# Temperature to use when generating text\n",
    "temp = 0.0\n",
    "\n",
    "# Fewshot\n",
    "few_shot = False\n",
    "\n",
    "# Separation between system and user prompt\n",
    "system_user_division = 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Unitily Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Weights and Biases run\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=WANDB_PROJECT_NAME,\n",
    "    name=run_name if run_name != '' else f'{MODEL_NAME}_ds_{dataset}_task_{int(task)}'\n",
    "                                                    f'_sample_{sample_size}_epochs_{n_epochs}'\n",
    "                                                    f'_full_label_names_{str(not not_use_full_labels)}'\n",
    "                                                    f'_temp_{temp}',\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"dataset\": dataset_num_to_dataset_name[int(dataset)],\n",
    "        \"task\": task_num_to_task_name[int(task)],\n",
    "        \"epochs\": n_epochs,\n",
    "        \"temp\": temp\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_idx, dataset_task_mappings = load_dataset_task_prompt_mappings(\n",
    "    dataset_num=dataset, task_num=task, dataset_task_mappings_fp=dataset_task_mappings_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information specific to the dataset\n",
    "label_column = dataset_task_mappings.loc[dataset_idx, \"label_column\"]\n",
    "system_prompt = dataset_task_mappings.loc[dataset_idx, 'zero_shot_prompt']\n",
    "user_prompt_format = dataset_task_mappings.loc[dataset_idx, 'user_prompt']\n",
    "\n",
    "#system_user_prompt_division_line = 3 if args.task != 3 else 15\n",
    "system_user_prompt_division_line = args.system_user_division\n",
    "system_prompt = ('\\n'.join(prompt.split('\\n')[:-system_user_prompt_division_line])).strip()\n",
    "user_prompt_format = ('\\n'.join(prompt.split('\\n')[-system_user_prompt_division_line:])).strip()\n",
    "print(user_prompt_format)\n",
    "\n",
    "\n",
    "# Log the system prompt and user_prompt_format as files in wandb\n",
    "prompts_artifact = wandb.Artifact('prompts', type='prompts')\n",
    "with prompts_artifact.new_file('system_prompt.txt', mode='w', encoding='utf-8') as f:\n",
    "    f.write(system_prompt)\n",
    "with prompts_artifact.new_file('user_prompt_format.txt', mode='w', encoding='utf-8') as f:\n",
    "    f.write(user_prompt_format)\n",
    "wandb.run.log_artifact(prompts_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_full_dataset(data_dir=args.data_dir, dataset_num=args.dataset, task_num=args.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information specific to the dataset\n",
    "label_column = dataset_task_mappings.loc[dataset_idx, \"label_column\"]\n",
    "if few_shot:\n",
    "    prompt = dataset_task_mappings.loc[dataset_idx, 'few_shot_prompt']\n",
    "else:\n",
    "    prompt = dataset_task_mappings.loc[dataset_idx, 'zero_shot_prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#system_user_prompt_division_line = 3 if args.task != 3 else 15\n",
    "    system_user_prompt_division_line = system_user_division\n",
    "    system_prompt = ('\\n'.join(prompt.split('\\n')[:-system_user_prompt_division_line])).strip()\n",
    "    user_prompt_format = ('\\n'.join(prompt.split('\\n')[-system_user_prompt_division_line:])).strip()\n",
    "    print(user_prompt_format)\n",
    "\n",
    "    # Log the system prompt and user_prompt_format as files in wandb\n",
    "    prompts_artifact = wandb.Artifact('prompts', type='prompts')\n",
    "    with prompts_artifact.new_file('system_prompt.txt', mode='w', encoding=\"utf-8\") as f:\n",
    "        f.write(system_prompt)\n",
    "    with prompts_artifact.new_file('user_prompt_format.txt', mode='w', encoding=\"utf-8\") as f:\n",
    "        f.write(user_prompt_format)\n",
    "    wandb.run.log_artifact(prompts_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_full_dataset(data_dir=data_dir, dataset_num=dataset, task_num=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_output_dir = os.path.join(\n",
    "    output_dir, 'preprocessed', 'full_name_labels' if not not_use_full_labels else 'single_letter_labels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in datasets.items():\n",
    "        print(df_name)\n",
    "        print(df.head())\n",
    "        df['completion_label'] = df[label_column].map(\n",
    "            lambda label: map_label_to_completion(label=label, task_num=task,\n",
    "                                                  full_label=not not_use_full_labels)\n",
    "        )\n",
    "        df['openai_instance_format'] = df.apply(\n",
    "            lambda row: create_training_example(\n",
    "                system_prompt=system_prompt, user_prompt_format=user_prompt_format,\n",
    "                user_prompt_text=row['text'],\n",
    "                completion=row['completion_label']\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        df['openai_instance_without_completion'] = df['openai_instance_format'].map(lambda x: x['messages'][:-1])\n",
    "\n",
    "        print(f'Check for errors {df_name} set: ')\n",
    "        assert not dataset_has_format_errors(df['openai_instance_format'].tolist()), f\"Errors found in {df_name}\"\n",
    "        os.makedirs(preprocessed_output_dir, exist_ok= True)\n",
    "        df.to_csv(os.path.join(preprocessed_output_dir, df_name + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create jsonl file and upload to OpenAI\n",
    "df_id_metadata = upload_datasets_to_openai(args, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_name = f'ds_{dataset}__task_{task}_train_set'\n",
    "model_name = (train_set_name.replace('__', '_')\n",
    "                .replace('train_set', 'trn')\n",
    "                .replace('task', 't')\n",
    "                .replace('_single_letter_labels', '_sl'))\n",
    "\n",
    "if args.few_shot:\n",
    "    model_name += \"_few_shot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the evaluation set and store the predictions\n",
    "print(\"\\n\" + \"#\" * 50)\n",
    "print(\"Getting predictions on the evaluation set\")\n",
    "predictions = []\n",
    "\n",
    "for messages in tqdm(eval_df['openai_instance_without_completion'].tolist()):\n",
    "    # Retry the completion at least COMPLETION_RETRIES times\n",
    "    num_retries = 0\n",
    "    response = None\n",
    "    while num_retries < COMPLETION_RETRIES and response is None:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "            model=full_model_name,\n",
    "            messages=messages,\n",
    "            temperature=args.temp,\n",
    "            n=1\n",
    "        )\n",
    "        except Exception as e:\n",
    "            print('Error getting predictions. Retrying...')\n",
    "            time.sleep(5)\n",
    "            num_retries += 1\n",
    "            if num_retries >= COMPLETION_RETRIES:\n",
    "                print('Maximum amount of retires reached')\n",
    "                raise e\n",
    "    predictions.append(response['choices'][0]['message']['content'])\n",
    "\n",
    "# Add predictions to df\n",
    "eval_df['prediction'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store output\n",
    "predictions_output_dir = os.path.join(output_dir, 'predictions',\n",
    "                                        f'dataset_{dataset}_task_{task}')\n",
    "os.makedirs(predictions_output_dir, exist_ok=True)\n",
    "#edited after running\n",
    "\n",
    "datasets[eval_set_name].to_csv(\n",
    "    os.path.join(predictions_output_dir, f\"{fmodel_name}--{run_name}.csv\"),\n",
    "    index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get performance metrics--\n",
    "y_true = eval_df['completion_label']\n",
    "y_pred = eval_df['prediction']\n",
    "\n",
    "label_type = 'full_name' if not not_use_full_labels else 'short_name'\n",
    "display_labels = task_to_display_labels[task][label_type]\n",
    "labels = display_labels\n",
    "\n",
    "cm_plot, classification_report, metrics = plot_count_and_normalized_confusion_matrix(\n",
    "    y_true, y_pred, display_labels, labels, xticks_rotation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log metrics\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    wandb.log({metric_name: metric_value})\n",
    "\n",
    "# Log the confusion matrix matplotlib figure\n",
    "wandb.log({'confusion_matrix': wandb.Image(cm_plot)})\n",
    "\n",
    "# Log the classification report as an artifact\n",
    "classification_report = (pd.DataFrame({k: v for k, v in classification_report.items() if k != 'accuracy'})\n",
    "                            .transpose().reset_index())\n",
    "wandb.log({'classification_report': wandb.Table(\n",
    "    dataframe=classification_report)})\n",
    "\n",
    "classification_report_artifact = wandb.Artifact(\n",
    "    f'classification_report_{model_name}', type='classification_report')\n",
    "\n",
    "with classification_report_artifact.new_file('classification_report.txt', mode='w') as f:\n",
    "    f.write(pprint.pformat(classification_report))\n",
    "\n",
    "wandb.run.log_artifact(classification_report_artifact)\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
