{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e33e3e52454241b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Template for Fine-Tuning Classification with FLAN through Low-Ranking Adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c444b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41710ff8",
   "metadata": {},
   "source": [
    "### Import all Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e9fe2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T14:40:48.409923Z",
     "iopub.status.busy": "2024-04-19T14:40:48.408333Z",
     "iopub.status.idle": "2024-04-19T14:40:48.421037Z",
     "shell.execute_reply": "2024-04-19T14:40:48.419828Z",
     "shell.execute_reply.started": "2024-04-19T14:40:48.409819Z"
    }
   },
   "outputs": [],
   "source": [
    "#IMPORTANT - to ensure package loading, first add the path of the utils folder to your system path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_dir = os.getcwd()\n",
    "sys.path.append(os.path.abspath(os.path.join(module_dir, os.pardir, \"utils\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a44e878ea21b79",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-04-19T14:40:48.423351Z",
     "iopub.status.busy": "2024-04-19T14:40:48.422734Z",
     "iopub.status.idle": "2024-04-19T14:41:00.404525Z",
     "shell.execute_reply": "2024-04-19T14:41:00.403903Z",
     "shell.execute_reply.started": "2024-04-19T14:40:48.423316Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mkorob/conda/envs/environment_cp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import time\n",
    "import random\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_int8_training\n",
    "from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
    "                          DataCollatorForSeq2Seq)\n",
    "\n",
    "from dataload_utils import load_train_and_eval_datasets, load_dataset_task_prompt_mappings, preprocess_function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c2aff",
   "metadata": {},
   "source": [
    "### Setup Arguments and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84d6532e4e9c2c8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    " In the following code block, you are required to set up several key parameters that will define the behavior and environment of your fine-tuning process:\n",
    "\n",
    "1. **WandB Project Name (`WANDB_PROJECT_NAME`)**: This is the name of the project in Weights & Biases (WandB) where your training run will be logged. WandB is a tool that helps track experiments, visualize data, and share insights. By setting the project name here, you ensure that all the metrics, outputs, and logs from your training process are organized under a single project for easy access and comparison. Specify a meaningful name that reflects the nature of your training session or experiment.\n",
    "\n",
    "2. **Model Name (`MODEL_NAME`)**: Here, you select the size of FLAN model that you wish to fine-tune. This notebook was ran and tested on (`google/flan-t5-xl`), which we found to be the best trade-off between computational power required to run the model and the accuracy of predictions. Full list of models is available at : https://huggingface.co/docs/transformers/model_doc/flan-t5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdbbe44f31c3622e",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-04-19T14:41:00.406068Z",
     "iopub.status.busy": "2024-04-19T14:41:00.405436Z",
     "iopub.status.idle": "2024-04-19T14:41:00.409299Z",
     "shell.execute_reply": "2024-04-19T14:41:00.408705Z",
     "shell.execute_reply.started": "2024-04-19T14:41:00.406038Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Specs WandB and Which Model you want to fine-tune\n",
    "WANDB_PROJECT_NAME = \"FLAN_template_1\"\n",
    "MODEL_NAME ='google/flan-t5-xl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff25819d",
   "metadata": {},
   "source": [
    "In the next code block, you are required to set up various configuration variables that will dictate how the inference processes are executed. These variables are crucial as they define the nature of the task, the data, and the specific behaviors during the model's training and evaluation.\n",
    "\n",
    "1. **Task (`task`)**: Specify the type of task you want to run inference on. The task is represented by an integer, with each number corresponding to a different type of task (e.g., 1, 2, 3, etc.). You must select from the predefined choices, which are typically mapped to specific NLP tasks or scenarios.\n",
    "\n",
    "2. **Dataset (`dataset`)**: Choose the dataset on which you want to run inference. Like tasks, datasets are identified by integers, and each number corresponds to a different dataset. Ensure that the dataset selected is relevant to the task at hand.\n",
    "\n",
    "3. **Output Directory (`output_dir`)**: Define the path to the directory where you want to store the generated samples. This is where the output of your training and inference processes will be saved.\n",
    "\n",
    "4. **Random Seed (`seed`)**: Setting a random seed ensures that the results are reproducible. By using the same seed, you can achieve the same outcomes on repeated runs under identical conditions.\n",
    "\n",
    "5. **Data Directory (`data_dir`)**: Specify the path to the directory containing the datasets you plan to use for training and evaluation.\n",
    "\n",
    "6. **Label Usage (`not_use_full_labels`)**: This boolean variable determines whether to use the full label descriptions or abbreviated labels during training and inference. Setting it to `False` means full labels will be used.\n",
    "\n",
    "7. **Dataset-Task Mappings File Path (`dataset_task_mappings_fp`)**: Define the path to the file containing mappings between datasets and tasks. This file is crucial for ensuring the correct dataset is used for the specified task.\n",
    "\n",
    "9. **Number of Epochs (`n_epochs`)**: Specify the number of epochs for training the model. An epoch refers to one complete pass through the entire training dataset.\n",
    "\n",
    "10. **Maximum prompt length (`max_prompt_len`)**: The maximum length of prompt in tokens to be taken as input before truncating the input. Longer input sequences require more computational power to run, so the shortest sequence required to capture the text is recommended.\n",
    "\n",
    "11. **Batch size (`batch-size`)**: Number of observations used in each training and validation batch. Larger batch size requires more computational memory as one batch needs to fit on one machine, but makes learning more stable. We found that for FLAN-XL, batch size of 8 was possible by taking batch size of 4 and accumulating results of 2 batches (see  (`gradient_accumulation_steps`) below)\n",
    "\n",
    "12. **Gradient accumulation steps (`gradient_accumulation_steps`)**: In a case where gradient accumulation steps is larger than 1, instead of updating the gradient after each batch, the gradient is updated after the sum of _n_ batches. This allows to train a model to learn on a larger global batch (_batch size_ * _gradient accumulation steps_) than the one that is able to fit on one machine.\n",
    "\n",
    "**Customizing for Your Own Tasks:**\n",
    "If you plan to run a custom task or use a dataset that is not predefined, you will need to make modifications to the `utils_src` file. This file contains all mappings for different datasets and tasks. Adding your custom task or dataset involves defining the new task or dataset number and specifying its characteristics and mappings in the `utils_src` file. This ensures that your custom task or dataset integrates seamlessly with the existing framework for training and inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0834ab85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T14:45:03.396264Z",
     "iopub.status.busy": "2024-04-19T14:45:03.394676Z",
     "iopub.status.idle": "2024-04-19T14:45:03.405277Z",
     "shell.execute_reply": "2024-04-19T14:45:03.404161Z",
     "shell.execute_reply.started": "2024-04-19T14:45:03.396175Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration Variables\n",
    "\n",
    "# Type of task to run inference on\n",
    "task = 1  # As defined in dataset_task_mappings.csv\n",
    "\n",
    "# Dataset to run inference on\n",
    "dataset = 1  # As defined in dataset_task_mappings.csv\n",
    "\n",
    "# Size of the sample to generate\n",
    "sample_size = '50' \n",
    "\n",
    "# Path to the directory to store the generated samples\n",
    "output_dir = '../../data'\n",
    "\n",
    "# Random seed to use\n",
    "seed = 2019\n",
    "\n",
    "# Path to the directory containing the datasets\n",
    "data_dir = '../../data'\n",
    "\n",
    "#Path to where the models are stored\n",
    "model_dir = \"../../models\"\n",
    "\n",
    "# Whether to use the full label\n",
    "not_use_full_labels = False\n",
    "\n",
    "# Path to the dataset-task mappings file\n",
    "dataset_task_mappings_fp = os.path.normpath(os.path.join(module_dir, '..', '..','dataset_task_mappings.csv'))\n",
    "\n",
    "#Maximum length of prompt to be taken by the model as input (check documentation for current maximum length)\n",
    "max_prompt_len = 4096\n",
    "\n",
    "# Number of epochs to train the model\n",
    "n_epochs = 3\n",
    "\n",
    "# Batch size (we finetuned the models using batch sizes of 4 multiplied by gradient accumulation size to 2, which considers a mega-batch of 8)\n",
    "batch_size = 4\n",
    "\n",
    "#Gradient accumulation size\n",
    "gradient_accumulation_steps = 2\n",
    "\n",
    "#run name - Optional Argument if you want it to be called something else than the default way (defined below)\n",
    "run_name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7306e73c-f30d-4a59-a589-cee0681b558c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T14:42:52.934998Z",
     "iopub.status.busy": "2024-04-19T14:42:52.934328Z",
     "iopub.status.idle": "2024-04-19T14:42:52.944221Z",
     "shell.execute_reply": "2024-04-19T14:42:52.943490Z",
     "shell.execute_reply.started": "2024-04-19T14:42:52.934969Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set_name = f'ds_{dataset_num}__task_{task_num}_train_set_{sample_size}'\n",
    "eval_set_name = f'ds_{dataset_num}__task_{task_num}_eval_set'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eee2505",
   "metadata": {},
   "source": [
    "### Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51c6d38a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T14:42:52.945770Z",
     "iopub.status.busy": "2024-04-19T14:42:52.945327Z",
     "iopub.status.idle": "2024-04-19T14:42:52.956133Z",
     "shell.execute_reply": "2024-04-19T14:42:52.955598Z",
     "shell.execute_reply.started": "2024-04-19T14:42:52.945743Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "        Prints the number of trainable parameters in the model.\n",
    "        \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21e0a4b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T14:42:52.979654Z",
     "iopub.status.busy": "2024-04-19T14:42:52.979190Z",
     "iopub.status.idle": "2024-04-19T14:42:52.992216Z",
     "shell.execute_reply": "2024-04-19T14:42:52.991750Z",
     "shell.execute_reply.started": "2024-04-19T14:42:52.979625Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_all_seeds(seed: int = 123):\n",
    "    # tf.random.set_seed(123)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    # Set seed with the `transformers` library\n",
    "    # set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3984d6e1",
   "metadata": {},
   "source": [
    "## Main Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c54aafd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T14:42:52.993659Z",
     "iopub.status.busy": "2024-04-19T14:42:52.993118Z",
     "iopub.status.idle": "2024-04-19T14:43:18.415542Z",
     "shell.execute_reply": "2024-04-19T14:43:18.414952Z",
     "shell.execute_reply.started": "2024-04-19T14:42:52.993630Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:p84r2jy9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">google/flan-t5-xl_ds_1_task_1_sample_50_epochs_3_prompt_max_len_4096_batch_size_4_grad_acc_2</strong> at: <a href='https://wandb.ai/maria-korobeynikova/FLAN_template_1/runs/p84r2jy9' target=\"_blank\">https://wandb.ai/maria-korobeynikova/FLAN_template_1/runs/p84r2jy9</a><br/> View project at: <a href='https://wandb.ai/maria-korobeynikova/FLAN_template_1' target=\"_blank\">https://wandb.ai/maria-korobeynikova/FLAN_template_1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240419_164139-p84r2jy9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:p84r2jy9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/mkorob/OpenSource-LLM-Practitioner-Guide/src/FLAN/wandb/run-20240419_164253-ni1muw8b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/maria-korobeynikova/FLAN_template_1/runs/ni1muw8b' target=\"_blank\">google/flan-t5-xl_ds_1_task_1_sample_50_epochs_3_prompt_max_len_4096_batch_size_4_grad_acc_2</a></strong> to <a href='https://wandb.ai/maria-korobeynikova/FLAN_template_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/maria-korobeynikova/FLAN_template_1' target=\"_blank\">https://wandb.ai/maria-korobeynikova/FLAN_template_1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/maria-korobeynikova/FLAN_template_1/runs/ni1muw8b' target=\"_blank\">https://wandb.ai/maria-korobeynikova/FLAN_template_1/runs/ni1muw8b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running exp: google/flan-t5-xl_ds_1_task_1_sample_50_epochs_3_prompt_max_len_4096_batch_size_4_grad_acc_2\n"
     ]
    }
   ],
   "source": [
    "exp_name = (run_name if run_name != '' else f'{MODEL_NAME}_ds_{dataset}_task_{int(task)}_sample_{sample_size}_'\n",
    "            f'epochs_{n_epochs}_prompt_max_len_{max_prompt_len}_'\n",
    "            f'batch_size_{batch_size}_grad_acc_{gradient_accumulation_steps}')\n",
    "# Initialize the Weights and Biases run\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=WANDB_PROJECT_NAME,\n",
    "    name=exp_name,\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"dataset\": dataset,\n",
    "        \"task\": task,\n",
    "        \"epochs\": n_epochs,\n",
    "        \"max_prompt_len\": max_prompt_len,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"gradient_accumulation_steps\": gradient_accumulation_steps\n",
    "    }\n",
    ")\n",
    "\n",
    "print('Running exp:', exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbbdca6",
   "metadata": {},
   "source": [
    "### Load Data and the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22685986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T14:43:18.416755Z",
     "iopub.status.busy": "2024-04-19T14:43:18.416474Z",
     "iopub.status.idle": "2024-04-19T14:43:19.949232Z",
     "shell.execute_reply": "2024-04-19T14:43:19.948509Z",
     "shell.execute_reply.started": "2024-04-19T14:43:18.416725Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Artifact prompts>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_col = 'zero_shot_prompt'\n",
    "dataset_idx, dataset_task_mappings = load_dataset_task_prompt_mappings(\n",
    "    dataset_num=dataset, task_num=task, dataset_task_mappings_fp=dataset_task_mappings_fp)\n",
    "\n",
    "# Get information specific to the dataset and the prompt\n",
    "label_column = dataset_task_mappings.loc[dataset_idx, \"label_column\"]\n",
    "labelset = dataset_task_mappings.loc[dataset_idx, \"labelset\"].split(\",\")\n",
    "labelset = [label.strip() for label in labelset]\n",
    "prompt = dataset_task_mappings.loc[dataset_idx, prompt_col]\n",
    "\n",
    "datasets = load_train_and_eval_datasets(\n",
    "        data_dir=data_dir, eval_set_name=eval_set_name, train_set_name=train_set_name, task_num=task,\n",
    "        label_column=label_column, labelset=labelset, full_label=False, sample_size=sample_size)\n",
    "\n",
    "# Log the system prompt and user_prompt_format as files in wandb\n",
    "prompts_artifact = wandb.Artifact('prompts', type='prompts')\n",
    "with prompts_artifact.new_file('prompt.txt', mode='w') as f:\n",
    "    f.write(prompt)\n",
    "wandb.run.log_artifact(prompts_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff697d92",
   "metadata": {},
   "source": [
    "### Define the model, tokenizers, data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1be20018",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T14:43:19.951499Z",
     "iopub.status.busy": "2024-04-19T14:43:19.950851Z",
     "iopub.status.idle": "2024-04-19T14:43:30.362475Z",
     "shell.execute_reply": "2024-04-19T14:43:30.361838Z",
     "shell.execute_reply.started": "2024-04-19T14:43:19.951468Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.70s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.96ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 32.75ba/s]\n"
     ]
    }
   ],
   "source": [
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, truncation_side=\"left\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, load_in_8bit=True, device_map=\"auto\")\n",
    "\n",
    "# Preprocess training and validation sets\n",
    "unnecessary_cols = datasets['eval'].column_names\n",
    "\n",
    "tokenized_dataset = datasets.map(\n",
    "    lambda x:\n",
    "    preprocess_function(tokenizer, prompt=prompt, df=x, label_column=label_column,\n",
    "                        max_length=max_prompt_len, padding=False),\n",
    "    batched=True, remove_columns=unnecessary_cols)\n",
    "\n",
    "# We want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=batch_size,\n",
    "    padding='longest'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a360a7",
   "metadata": {},
   "source": [
    "### Prepare the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f551fdf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T14:45:10.175369Z",
     "iopub.status.busy": "2024-04-19T14:45:10.174279Z",
     "iopub.status.idle": "2024-04-19T14:45:10.408227Z",
     "shell.execute_reply": "2024-04-19T14:45:10.407554Z",
     "shell.execute_reply.started": "2024-04-19T14:45:10.175302Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mkorob/conda/envs/environment_cp/lib/python3.10/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9437184 || all params: 2859194368 || trainable%: 0.33006444422319176\n"
     ]
    }
   ],
   "source": [
    "# PeFT\n",
    "model = prepare_model_for_int8_training(model)\n",
    "lora_config = LoraConfig(\n",
    "    r=16, lora_alpha=32, target_modules=[\"q\", \"v\"], lora_dropout=0.05, bias=\"none\", task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    num_train_epochs=n_epochs,\n",
    "    load_best_model_at_end=False,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"eval\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f300320e",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d827a1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T14:45:10.410134Z",
     "iopub.status.busy": "2024-04-19T14:45:10.409494Z",
     "iopub.status.idle": "2024-04-19T14:51:05.976648Z",
     "shell.execute_reply": "2024-04-19T14:51:05.975879Z",
     "shell.execute_reply.started": "2024-04-19T14:45:10.410103Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/data/mkorob/conda/envs/environment_cp/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 02:40, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mkorob/conda/envs/environment_cp/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/data/mkorob/conda/envs/environment_cp/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained!, now merging weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.13s/it]\n",
      "/data/mkorob/conda/envs/environment_cp/lib/python3.10/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model merged with adapters in: ../../modelsgoogle/flan-t5-xl_ds_1_task_1_sample_50_epochs_3_prompt_max_len_4096_batch_size_4_grad_acc_2/merged_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.30s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "path_model_dir = model_dir + exp_name\n",
    "trainer.save_model(path_model_dir)\n",
    "print(\"Model trained!, now merging weights...\")\n",
    "\n",
    "# Merge weights and save the model\n",
    "flan = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "flan = prepare_model_for_int8_training(flan)\n",
    "\n",
    "model = PeftModel.from_pretrained(flan, path_model_dir)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "merged_model_path = os.path.join(path_model_dir, \"merged_model\")\n",
    "print(f\"saving model merged with adapters in: {merged_model_path}\")\n",
    "model.save_pretrained(merged_model_path, safe_serialization=True)\n",
    "\n",
    "# Free up memory\n",
    "del flan\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "time.sleep(10)\n",
    "\n",
    "# Reload quantized model with the merged weights of the adapter\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    merged_model_path, load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b28214",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a719abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dir = os.path.join(output_dir, 'predictions', 'flan', MODEL_NAME.replace('/', '_'))\n",
    "os.makedirs(predictions_dir, exist_ok=True)\n",
    "\n",
    "dataloader = DataLoader(tokenized_dataset['eval'], batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions_out = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        outputs = model.generate(\n",
    "            input_ids = \n",
    "            batch['input_ids'].cuda(),\n",
    "        )\n",
    "\n",
    "        generated_text_minibatch = tokenizer.batch_decode(\n",
    "            outputs, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "\n",
    "        predictions_out += generated_text_minibatch\n",
    "\n",
    "        if i == 0:\n",
    "            print(\"Sample prediction: \")\n",
    "            print(predictions_out[0])\n",
    "\n",
    "eval_df = pd.read_csv(os.path.join(data_dir, f'{eval_set_name}.csv'))\n",
    "\n",
    "eval_df['prediction'] = predictions_out\n",
    "print(eval_df.head())\n",
    "eval_df.to_csv(os.path.join(predictions_dir, f'{exp_name.replace(\"/\", \"_\")}.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdbb095",
   "metadata": {},
   "source": [
    "### Terminate WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33d3f653",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T14:52:30.659443Z",
     "iopub.status.busy": "2024-04-19T14:52:30.658009Z",
     "iopub.status.idle": "2024-04-19T14:52:35.798116Z",
     "shell.execute_reply": "2024-04-19T14:52:35.797522Z",
     "shell.execute_reply.started": "2024-04-19T14:52:30.659381Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>2.77</td></tr><tr><td>train/global_step</td><td>18</td></tr><tr><td>train/total_flos</td><td>709566815010816.0</td></tr><tr><td>train/train_loss</td><td>0.81493</td></tr><tr><td>train/train_runtime</td><td>164.029</td></tr><tr><td>train/train_samples_per_second</td><td>0.914</td></tr><tr><td>train/train_steps_per_second</td><td>0.11</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">google/flan-t5-xl_ds_1_task_1_sample_50_epochs_3_prompt_max_len_4096_batch_size_4_grad_acc_2</strong> at: <a href='https://wandb.ai/maria-korobeynikova/FLAN_template_1/runs/ni1muw8b' target=\"_blank\">https://wandb.ai/maria-korobeynikova/FLAN_template_1/runs/ni1muw8b</a><br/> View project at: <a href='https://wandb.ai/maria-korobeynikova/FLAN_template_1' target=\"_blank\">https://wandb.ai/maria-korobeynikova/FLAN_template_1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240419_164253-ni1muw8b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment_new",
   "language": "python",
   "name": "environment_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
