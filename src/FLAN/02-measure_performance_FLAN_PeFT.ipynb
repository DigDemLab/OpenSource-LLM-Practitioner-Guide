{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21bbcb7a",
   "metadata": {},
   "source": [
    "# Template for Accuracy Calculation for FLAN zero-shot and finetuned models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3247fa77",
   "metadata": {},
   "source": [
    "This script allows you to calculate accuracy scores for the results of fine-tuned and zero-shot models. Especially for zero-shot predictions, you need to re-map the output of the model to match with the labels of the original data to calculate accurate metrics, as the output might add additional words or rephrase the output. If your task is not one of the tasks that we provide, you might need to add the mapping functions yourself by inspecting the output of your predictions (e.g. you can print all the unique combinations of output and original labels to see which categories have been created.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b1272b",
   "metadata": {},
   "source": [
    "### Import all Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T11:57:32.100951389Z",
     "start_time": "2023-10-03T11:57:31.795989172Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "import pandas as pd \n",
    "from IPython.core.display import Markdown\n",
    "\n",
    "from dataload_utils import  load_dataset_task_prompt_mappings\n",
    "from label_utils import plot_count_and_normalized_confusion_matrix, map_label_to_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d741d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dfaa57",
   "metadata": {},
   "source": [
    "### Define Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1013111d",
   "metadata": {},
   "source": [
    "These functions map the outputs generated by FLAN to the original labels for each task. Adjust the functions or add your own as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37cb94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_outputs_task_1(output):\n",
    "    if re.search(r'^(answer:){0,1}(\\s)*a(\\s)*$|(a(\\.|:|\\)))|(\\s|^|\\')relev(a|e)nt|aelevant', output.lower().strip()):\n",
    "        return 'A'\n",
    "    elif re.search(r'^(answer:){0,1}(\\s)*b(\\s)*$|b(\\.|:|\\))|not relevant|irrelevant|ielevant|\\s+b$|brrelevant', output.lower().strip()):\n",
    "        return 'B'\n",
    "    elif output == np.nan or output == 'nan':\n",
    "        return \"\"\n",
    "    else:\n",
    "        print(f'Weird value: {output.lower().strip()}')\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8124fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_outputs_task_2(output):\n",
    "    if re.search(r'^(answer:){0,1}(\\s)*a(\\s)*$|a(\\.|:|\\))|challnge|problem|\\bpro\\b|blem', output.lower().strip()):\n",
    "        return 'A'\n",
    "    elif re.search(r'^(answer:){0,1}(\\s)*b(\\s)*$|b(\\.|:|\\))|solution|\\blution\\b', output.lower().strip()):\n",
    "        return 'B'\n",
    "    elif re.search(r'^(answer:){0,1}(\\s)*c(\\s)*$|c(\\.|:|\\))|neither|neutral|(\\s)+c$', output.lower().strip()):\n",
    "        return 'C'\n",
    "    elif output == np.nan or output == 'nan':\n",
    "        return \"\"\n",
    "    else:\n",
    "        print(f'Weird value: {output.lower().strip()}')\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f319c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_outputs_task_3(output):\n",
    "    if re.search(r'^(answer:){0,1}(\\s)*a(\\s)*$|a(\\.|:|\\))|economic|economy|aconomy', output.lower().strip()):\n",
    "        return 'A'\n",
    "\n",
    "    elif re.search(r'^(answer:){0,1}(\\s)*b(\\s)*$|b(\\.|:|\\))|morality|rality', output.lower().strip()):\n",
    "        return 'B'\n",
    "\n",
    "    elif re.search(r'^(answer:){0,1}(\\s)*c(\\s)*$|c(\\.|:|\\))|fairness and equality|irness and equality', output.lower().strip()):\n",
    "        return 'C'\n",
    "\n",
    "    elif re.search(r'^(answer:){0,1}(\\s)*d(\\s)*$|d(\\.|:|\\))|policy prescription and evaluation|prescription and evaluation|licy prescription',\n",
    "                   output.lower().strip()):\n",
    "        return 'D'\n",
    "\n",
    "    elif re.search(r'^(answer:){0,1}(\\s)*e(\\s)*$|e(\\.|:|\\))|law and order|crime and justice|law enforcement|w and order', output.lower().strip()):\n",
    "        return 'E'\n",
    "\n",
    "    elif re.search(r'^(answer:){0,1}(\\s)*f(\\s)*$|f(\\.|:|\\))|security and defense|curity and defense', output.lower().strip()):\n",
    "        return 'F'\n",
    "\n",
    "    elif re.search(r'^(answer:){0,1}(\\s)*g(\\s)*$|g(\\.|:|\\))|health and safety|alth and safety', output.lower().strip()):\n",
    "        return 'G'\n",
    "\n",
    "    elif re.search(r'^(answer:){0,1}(\\s)*h(\\s)*$|h(\\.|:|\\))|quality of life|ality of life', output.lower().strip()):\n",
    "        return 'H'\n",
    "\n",
    "    elif re.search(r'^(answer:){0,1}(\\s)*i(\\s)*$|i(\\.|:|\\))|political|litical', output.lower().strip()):\n",
    "        return 'I'\n",
    "\n",
    "    elif re.search(r'^(answer:){0,1}(\\s)*j(\\s)*$|j(\\.|:|\\))|external (regulation|region) and reputation|external regulation|regulation and reputation', output.lower().strip()):\n",
    "        return 'J'\n",
    "\n",
    "    elif re.search(\n",
    "            r'^(answer:){0,1}(\\s)*k(\\s)*$|(k|n|w)(\\.|:|\\))|other|climate change|leadership and executive responsibility|'\n",
    "            r'expansion of service opportunities|access to higher ed|potential',\n",
    "            output.lower().strip()):\n",
    "        return 'K'\n",
    "\n",
    "    elif output == np.nan or output == 'nan':\n",
    "        return \"\"\n",
    "\n",
    "    else:\n",
    "        print(f'Weird value: {output.lower().strip()}')\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e6ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_outputs_task_4(output):\n",
    "    if re.search(r'^(answer:){0,1}(\\s)*a(\\s)*$|a(\\.|:|\\))|positive|postive stance|in favor|in advantage of|aast|a favor of a', output.lower().strip()):\n",
    "        return 'A'\n",
    "\n",
    "    elif re.search(r'^(answer:){0,1}(\\s)*b(\\s)*$|b(\\.|:|\\))|negative|negative stance|against|aggainst|bast', output.lower().strip()):\n",
    "        return 'B'\n",
    "\n",
    "    elif re.search(r'^(answer:){0,1}(\\s)*c(\\s)*$|c(\\.|:|\\))|neutral|neutral stance|cast', output.lower().strip()):\n",
    "        return 'C'\n",
    "\n",
    "    elif output == np.nan or output == 'nan':\n",
    "        return \"\"\n",
    "\n",
    "    else:\n",
    "        print(f'Weird value: {output.lower().strip()}')\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54df287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_outputs_task_5(output):\n",
    "    if re.search(r'^(answer:){0,1}(\\s)*a(\\s)*$|a(\\.|:|\\))|section 230|230', output.lower().strip()):\n",
    "        return 'A'\n",
    "\n",
    "    elif re.search(r'^(answer:){0,1}(\\s)*b(\\s)*$|b(\\.|:|\\))|trump ban|ban donald trump|ban(ning){0,1} trump|tr ban', output.lower().strip()):\n",
    "        return 'B'\n",
    "\n",
    "    elif re.search(r'^(answer:){0,1}(\\s)*c(\\s)*$|c(\\.|:|\\))|twitter support', output.lower().strip()):\n",
    "        return 'C'\n",
    "\n",
    "    elif re.search(r'^(answer:){0,1}(\\s)*d(\\s)*$|d(\\.|:|\\))|platform policies|policies', output.lower().strip()):\n",
    "        return 'D'\n",
    "\n",
    "    elif re.search(r'^(answer:){0,1}(\\s)*e(\\s)*$|e(\\.|:|\\))|complaint(s)+', output.lower().strip()):\n",
    "        return 'E'\n",
    "\n",
    "    elif re.search('^(answer:){0,1}(\\s)*f(\\s)*$|f(\\.|:|\\))|other',\n",
    "                   output.lower().strip()):\n",
    "        return 'F'\n",
    "\n",
    "    elif output == np.nan or output == 'nan':\n",
    "        return \"\"\n",
    "\n",
    "    else:\n",
    "        print(f'Weird value: {output.lower().strip()}')\n",
    "        return  \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0566a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_outputs_task_6(output):\n",
    "     if re.search(r'^(answer:){0,1}(\\s)*a(\\s)*$|a(\\.|:|\\))|policy prescription|policy prescription and regulation|licy and regulation|alicy',\n",
    "                   output.lower().strip()):\n",
    "        return 'A'\n",
    "     \n",
    "     elif re.search(r'^(answer:){0,1}(\\s)*b(\\s)*$|b(\\.|:|\\))|morality|rality', output.lower().strip()):\n",
    "        return 'B'\n",
    "     \n",
    "     elif re.search(r'^(answer:){0,1}(\\s)*c(\\s)*$|c(\\.|:|\\))|economics|econom|onomics', output.lower().strip()):\n",
    "        return 'C'\n",
    "\n",
    "     elif re.search(r'^(answer:){0,1}(\\s)*d(\\s)*$|d(\\.|:|\\))|other', output.lower().strip()):\n",
    "        return 'D'\n",
    "\n",
    "     elif output == np.nan or output == 'nan':\n",
    "        return \"\"\n",
    "\n",
    "     else:\n",
    "        print(f'Weird value: {output.lower().strip()}')\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6ab0cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output_completed(completion: str, task:int) -> str:\n",
    "    completion = re.sub(r'(?i)Answer|folks|Plain|River|IN', '', completion)    \n",
    "    answers = completion.strip().split(' ')\n",
    "    if task == 1:\n",
    "        return map_outputs_task_1(completion)\n",
    "    if task == 2:\n",
    "        return map_outputs_task_2(completion)\n",
    "    if task == 3:\n",
    "        return map_outputs_task_3(completion)\n",
    "    if task == 4:\n",
    "        return map_outputs_task_4(completion)\n",
    "    if task == 5:\n",
    "        return map_outputs_task_5(completion)\n",
    "    if task == 6:\n",
    "        return map_outputs_task_6(completion)\n",
    "    \n",
    "    #YOU MIGHT NEED TO ADD YOUR TASK HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8604f3",
   "metadata": {},
   "source": [
    "### Set Up Arguments and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f76c19",
   "metadata": {},
   "source": [
    "Here, you define the values for the dataset that you are loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb32ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration variables\n",
    "\n",
    "#Name of the file to compute accuracy for\n",
    "prediction_file_name = \"data/predictions/chatgpt/ds_1_t_1_file.csv\"\n",
    "\n",
    "#Path to the dataset-task mappings file\n",
    "dataset_task_mappings_fp = os.path.normpath(os.path.join('..', '..', 'dataset_task_mappings.csv'))\n",
    "\n",
    "# Type of task to run inference on\n",
    "task = 1  # Choices: [1,2,3,4,5,6]\n",
    "\n",
    "# Dataset to run inference on\n",
    "dataset = 1  # \n",
    "\n",
    "# Size of the sample to generate\n",
    "sample_size = '250'  # Enter 0 for zero-shot predictions\n",
    "\n",
    "# Zero-shot\n",
    "zero_shot = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9139db12b6e3f8e5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Main Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dcfe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file with predictions\n",
    "df = pd.read_csv(prediction_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbb7785b19d8bf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T12:05:07.264593304Z",
     "start_time": "2023-10-03T12:03:55.766727279Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the mappings\n",
    "dataset_task_mappings_fp = pd.read_csv(dataset_task_mappings_fp)\n",
    "\n",
    "# Get the expected labelset\n",
    "dataset_idx, dataset_task_mappings = load_dataset_task_prompt_mappings(\n",
    "    dataset_num=dataset, task_num=task, dataset_task_mappings_fp=dataset_task_mappings_fp)\n",
    "label_column = dataset_task_mappings.loc[dataset_idx, \"label_column\"]\n",
    "labelset = dataset_task_mappings.loc[dataset_idx, \"labelset\"].split(\",\")\n",
    "labelset = [label.strip() for label in labelset]\n",
    "labelset_full_description = dataset_task_mappings.loc[dataset_idx, \"labelset_fullword\"].split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27982118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions and map answers as in the functions above\n",
    "y_pred = df.prediction_ds.map(lambda x: process_output_completed(x, task))\n",
    "\n",
    "# Get ground truth in same format (mappings defined in label_utils)\n",
    "y_true = df[label_column].map(lambda label: map_label_to_completion(label=label, task_num=task, full_label=False))\n",
    "assert y_true.map(lambda pred: pred not in labelset).sum() == 0, 'Ground truth not in expected labelset'\n",
    "    \n",
    "# Get accuracy\n",
    "labels = labelset\n",
    "display_labels = labelset_full_description\n",
    "cm_plot, classification_report, metrics = plot_count_and_normalized_confusion_matrix(\n",
    "    y_true, y_pred, display_labels, labels, xticks_rotation='horizontal')\n",
    "\n",
    "# Get accuracy and other metrics\n",
    "print({\n",
    "    'sample_size': sample_size,\n",
    "    'accuracy': metrics['accuracy'],\n",
    "    'f1-macro': metrics['f1'],\n",
    "    'precision': metrics['precision'],\n",
    "    'recall': metrics['recall']\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
